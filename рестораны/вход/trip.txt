Abstract—The impact of customer reviews on user purchase decisions has been well documented. For example, a one-star increase in a restaurant’s Yelp rating can lead to a 5 to 9 percent increase in revenue [1]. Unfortunately, this has motivated some businesses in review-dependent industries to falsify reviews. In the era of big data, analytical methods have made detection of these false reviews easier. We perform a longitudinal study of 2.65 million hotel reviews made by nearly 320,000 reviewers on TripAdvisor (which does not verify customers stayed at the property they reviewed) and compare them to 2.93 million reviews on other two other booking platforms, Agoda and Booking.com (which verify its reviewers stayed at least one night at the property they reviewed). We analyze the language used, the patterns of reviewer activity, and the change in hotel’s reputation score over time across more than 5.5 million reviews. We find the word frequency between the two types of websites and the patterns of reviewer activity differ considerably, even though the relative ranking of hotel reputation scores across review platforms are similar.
Keywords—opinion spam, online reviews, crowd wisdom, big data, linguistic analysis, fake reviews, consumer generated content
I. INTRODUCTION
Consumer-created reviews of products and services are a critical driver of everyday decision-making. The credibility of these reviews is damaged when businesses commit review fraud, either creating fake positive reviews for themselves (boosting) or negative reviews for their competitors (vandalizing). This fraud is a significant concern of many review platforms such as TripAdvisor, and nearly all take an active role in vetting reviews.
TripAdvisor began in 2000 and quickly built a following as a “trusted source of travel information”. According to comScore as of April 2018, TripAdvisor was the 36th most popular website in the United States with 76 million unique monthly visitors (as identified by IP address) [2]. In 2017, one in every 11 travelers worldwide visited TripAdvisor in July, typically the brand's most popular month. It equates to 320 million unique visits a month - triple the volume TripAdvisor achieved during July only three years earlier [3].
Once TripAdvisor gained a dominant share of the travel market, the number of fake reviews grew as well. Instead of taking measures to verify and guarantee the reliability of the reviews, TripAdvisor changed the “trust” slogan to “the
978-1-5386-9125-0/18/$31.00 ©2018 IEEE DOI 10.1109/ICBK.2018.00040
biggest” and began to focus on revenue as a booking engine and charging companies for preferential visibility. Many argue the lack of transparency hurts travelers because most travelers believe businesses are fairly represented on TripAdvisor.
Despite the commitment of hotel review platforms to combat fraudulent reviews, the percentage of fake reviews on websites that do not verify purchases is estimated to be between 1030% [4]. According to Luka and Zervas [5], the number of reviews flagged as suspicious by Yelp, another source of travel information that does not verify consumer visits, is estimated at 16% and has been increasing. Although TripAdvisor claims approximately 50 filters in its vetting process [4], our research shows that websites that do not verify hotel stays (such as TripAdvisor) have a similar rate of fraudulent reviews as Yelp. This raises the question: How are fake reviewers (spammers) able to boost and vandalize hotels?
An entire industry of reputation management companies has been created to craft highly believable fake reviews through boosting, “reputation fixing” after receiving bad reviews, or sabotaging their competitors through vandalism. In 2015, to expose how prevalent fake reviews can be, the Italian magazine Italia a Tavola created a fictitious restaurant on TripAdvisor. The restaurant secured the number one rank for restaurants in its region using a boosting approach [4]. This came a year after TripAdvisor was issued a 500,000 Euro fine by the Italian Competition Authority (ICA) for failing to adopt controls against false reviews while promoting its content as “authentic and genuine” [6]. In April 2017, journalist Oobah Butler created a fake restaurant in a backyard shed – the “Shed at Dulwich” – that rose to become TripAdvisor’s top-rated London restaurant (out of more than 18,000) by December 2017 despite never having served a dish [7]. Butler stated that one of his motivations was the number of fake reviews he was paid to write for restauranteurs on TripAdvisor despite never having eaten in their restaurants [8].
Other websites that provide hotel reviews, such as Agoda and Booking.com, only allow customers who booked through their website to leave reviews. We call these reviews of verified stays. As techniques to analyze large datasets from heterogeneous sources become more ubiquitous, it becomes easier to not only cross-examine reviews to detect patterns of fraud but also identify websites that do a poor job detecting fraud. In this paper, we analyze TripAdvisor hotel reviews using these techniques and examine how they differ from
243
2018 IEEE International Conference on Big Knowledge
Decomposing TripAdvisor: Detecting Potentially Fraudulent Hotel Reviews in the Era of Big Data
Christopher G. Harris Department of Computer Science University of Northern Colorado Greeley, Colorado USA christopher.harris@unco.edu

reviews on websites with verified stays. We examine the language used in reviews, the patterns of reviewer activity, the features of the reviews themselves, and the changes in the hotels’ review score. If they differ considerably, this may call into question the utility of reviews found on that do not provide verification of actual stays.
The remainder of this paper is organized as follows. In Section 2 we cover relevant previous work in detecting fake reviews and establish the motivation behind our study. We briefly introduce the focus of our study in Section 3 and our dataset in Section 4. In Section 5, we describe our evaluation process. We conclude our study with analysis, discuss the limitations of our study, and describe future work in Section 6.
II. BACKGROUNDANDMOTIVATION
Opinion spam, defined by Ott et. al. [9] as “inappropriate or fraudulent reviews”, has been examined by many researchers. Most approaches used to detect deceptive opinion spam, or fake reviews, have analyzed the language used in the review’s text. In [10] Sussin and Thompson indicates fake reviews. are often shorter, and have more extreme sentiment. Another sign is reviews come from the same IP address (or region) over a short duration of time. Using a greater number of superlatives or adverbs also is a red flag for potential review fraud.
Ott et. al. [9] looked at deceptive opinion spam by using n- gram classifiers to examine deceptive opinion spam on TripAdvisor. They took 400 gold standard reviews from TripAdvisor and had the crowd create 400 false reviews and found that their model achieved a high accuracy classifying the pooled set of 800 reviews into real or fake reviews. Yoo and Gretzel [11] manually compared the psychologically relevant linguistic differences between their features for 40 genuine and 42 deceptive hotel reviews. Harris [12] took bodybuilding supplement reviews from websites, had the crowd create 400 false reviews, and then used the features in [11] to detect fake reviews with an accuracy of 75%. The challenge with each of these three approaches is they assume the “gold standard” reviews taken from the websites are indeed genuine when they are themselves often polluted with false reviews. Mukherjee et. al. [13] examined the language used in the reviews in addition to other features. They replicated Ott’s crowdsourced dataset and scraped some websites of Yelp data and demonstrated, through behavior analysis, that crowdsourced datasets are inferior to real spammers.
Jindal and Liu [14] examined 5.8 million reviews and 2.14 million reviewers (members who wrote at least one review) on Amazon.com. To examine the first type (untruthful reviews) they identify features to detect potentially fake reviews. Shojaee et. al. [15] detected deceptive opinion spam through machine learning techniques using lexico-syntactic patterns, and part-of speech (POS) tags and features, derived from the LIWC (Linguistic Inquiry and Word Count) output, a popular text analysis tool used in the social sciences to detect deception. We use contributions from these two studies in our analysis of sentiment and keywords in hotel reviews.
Spammers try to vary their writing style and verbal usage. To distinguish truthful reviews from fake reviews, Burgoon et
al. [16] found that there are several linguistic clues. For instance, spammers use simpler, short, and fewer average syllables per word in comparison with real reviewers. We also explore these claims in our study. Fei et.al. [17] looked at burstiness, or the arrival pattern of reviews to TripAdvisor’s website. They found that if there is a rapid arrival of TripAdvisor reviews for a hotel that normally experiences a slower pattern of reviews, particularly if they are extreme, this potentially signals a boosting or vandalism campaign.
Some researchers, with the assistance of review data, have taken a holistic view of the causes of reviewer fraud. Luka and Zervas [5] analyzed restaurant reviews identified by Yelp’s filtering algorithm as suspicious and treated them as a proxy for review fraud. They found that contributors whose reviews were flagged as potentially fraudulent had written fewer reviews than those whose reviews were published and had fewer friends on Yelp's social network. As with [10], these same researchers found reviews identified as suspicious tend to be contain stronger sentiment (either positive or negative) than other reviews. Additionally, restaurants with a weak reputation or that were independent (i.e., not part of a chain) were more likely to commit review fraud. These same findings are likely to apply to other websites that do not verify purchases.
Few studies to date have compared the prevalence of fake reviews on websites that do not verify actual purchases with those that do. Mayzlin et. al [18] compared reviews on TripAdvisor and Expedia.com. They scrape data from both websites and find that independent hotels engaged in more potential fraud than chain hotels. We borrow many of the

Colorado Springs, USA Dublin, Ireland Hong Kong London, UK Miami, USA Mumbai, India New Orleans, USA New York, USA Singapore Sydney, Australia Toronto, Canada Washington, DC, USA
Total Reviewed Agoda
400 600
TripAdvisor Booking.com
800 1000
0 200
Fig. 1. Number of hotels (top) and hotel reviews (bottom) evaluated from each market in our study.
 
Colorado Springs, USA Dublin, Ireland Hong Kong London, UK Miami, USA Mumbai, India New Orleans, USA New York, USA Singapore Sydney, Australia Toronto, Canada Washington, DC, USA
TripAdvisor Agoda Booking.com
400 600 Thousands
0 200
244

approaches Mayzlin et.al., used to identify potentially fake reviews in our study.
III. MODELING FRAUDULENT REVIEWS
Fraudulent reviews are notoriously challenging to detect. Ideally, if we could identify and model them, the linear regression formula to detect fake reviews would be:
rit = xit β + bi + σt+ εit; (i=1...N; t=1...T); (1)
where rit is the number of fake reviews business i received during time period t, xit is a vector of time-varying covariates measuring business i's economic incentives to engage in review fraud, β are the structural parameters of interest, σt and bi are business and time mixed effects, and the εit is an error term. The challenge we have in our study is identifying the values xit and β in Eq. 1. While xit may be dependent on each hotel and
10 9 8 gnita 7 Rm 6 oc.g 5 nik 4 o oB 3
2 1 0
5 4.5
each individual market, β indicate features we hope to uncover in this study as important to distinguish fake reviews from genuine ones.
IV. DATASET
We scraped data for 12 markets from TripAdvisor, Booking.com and Agoda on a quarterly basis from May 2016 to May 2018. We limited our data to hotels with at least 10 reviews in English in each market. Properties needed to have at least 5 contributed reviews per year to be considered in our longitudinal study. We exclude those reviews written in languages other than English, which made up 6.1% of the total. We also exclude those that provide a rating score but no text in the review. We did not include other property types (apartments, bed and breakfasts, guest houses or hostels) in our study – many appeal to a very different clientele and some of
(b)
(a)
10 9
87
(c)
(d)
5
(e)
5
gn4.5
itaR 4
rosiv3.5
dApi 3
rTe2.5 ga
1A1 0 10 20 30 40 50 60 70 80 90
TripAdvisor Ranking
gni 4
gn4
ta 3 . 5 R
ita Rr osi 3 vdAp2.5
ro siv 3 dApi 2 . 5 rT2
3.5
1.5
irT 2 1.5 1
0 10 20 30 40 50 60 70 80 90 TripAdvisor Ranking
2 1 0
4.5
0 10 20 30 40 50 60 70 80 90 TripAdvisor Ranking
1 2 3 4 5 6 7 8 9 10
2 3 4 5 6 7 8 9 10 Agoda Ranking
Booking.com Rating
iT 2 Ae
rev 2 1.5 garev1.5
Fig. 2. Correlation between (a) Booking.com rating & TripAdvisor ranking, (b) Agoda rating & TripAdvisor ranking, (c) Booking.com rating & TripAdvisor rating, (d) Agoda rating & TripAdvisor rating, (e) all TripAdvisor ratings & TripAdvisor ranking, and (f) TripAdvisor ratings for 2017-18 & TripAdvisor ranking. Data is shown for Colorado Springs only for clarity; all 12 markets we evaluated followed a similar pattern.
245
(f)
)85 1-714.5
02( 4 gnita3.5
gnit 6
aRa 5
dog 4 A 3
Rro 3
sivd2.5 Ap
0 10 20 30 40 50 60 70 80 90 TripAdvisor Ranking
their hosts are part-time, making a longitudinal study challenging. We matched hotels from TripAdvisor, Agoda, and Booking.com on property name; for those where we found no match, we manually associated them based on their physical address using data from Smith Travel Research, a market research firm that provides data to the hotel industry.
We include 3050 hotels which satisfy these conditions, have a presence on TripAdvisor, and have a presence on either Agoda or Booking.com. Although TripAdvisor is the most comprehensive review website, some hotels do not appear on TripAdvisor because they do not satisfy the minimum number of reviews criteria, they went out of business during the two- year period we examined, or because TripAdvisor defines the boundaries of some cities differently than Agoda and/or Booking.com. These 3050 hotels provide us with 2.65 million TripAdvisor reviews and 2.93 million reviews from Agoda and Booking.com. Figure 1 shows the number of hotels for each market that were used from each source.
Agoda and Booking,com, two hotel booking websites owned by Booking Holdings N.V., only allow customers who booked a hotel through their websites to rate and review the property. Unlike TripAdvisor, where reviewers provide an overall score on a five-point scale and a separate score for each feature of the hotel (service, sleep quality and location), Agoda and Booking.com have customers score each feature (host(s), facilities, cleanliness, comfort, value for money, and location) separately, and these scores are averaged together to create a cumulative rating score on a ten-point scale.
Agoda and Booking.com do not rank properties. However, the scores for Agoda and Booking.com correlate well with TripAdvisor rankings (r=-0.832 and r=-0.838, respectively) as well as with the TripAdvisor average scores (r=0.887 and r=0.872, respectively). We provide an illustration for Colorado Springs in Figure 2. This correlation was the same across all 12 markets we studied.
A. LanguageAnalysis
Using the NLP toolkit [19], a sentiment analyzer that uses 2 classifiers trained on the movie dataset created by Pang and Lee [20]. Pang et al. [21] reported that unigrams outperform bigrams when performing the sentiment classification of movie reviews, and Dave et al. [22] have obtained contrary results: n- grams (bigrams and trigrams) worked better for the product- review polarity classification. We experimented with unigrams and bigrams and report on our findings in Section V.B.2 of this paper. We also look at the neutrality and sentiment polarity qualities of the review, scoring it as positive, neutral, or negative. Moreover, we look at what people are raving or complaining about by extracting keywords from the review and analyze the related sentiment for these keywords. Last, we also look at the keywords that are mentioned most frequently for each hotel and for each city, looking for differences and compare the frequency of these terms between reviewers of suspicious hotels on TripAdvisor, reviewers of non-suspicious hotels on TripAdvisor, and reviewers on websites which verify its reviewers: Agoda and Booking.com.
V. HOTELREVIEWPATTERNS
There are three primary types of information related to a hotel review: (1) the review content, (2) the reviewer who wrote the review, and (3) the product (in this case a hotel room) being reviewed. Therefore, we have three types of features to examine: (1) review-centric features, (2) reviewer- centric features, and (3) product-centric features.
A. Reviews
TripAdvisor not only displays an average rating for each hotel on a scale of 1 to 5 stars (in half-bubble increments), it also claims to rank hotels based on a number of additional factors that remain proprietary. When all TripAdvisor reviews for a property are averaged, it correlates strongly with property ranking (r=-0.914), i.e., the lower the numeric rank, the higher the average rating. When we look at reviews since January 2017, the correlation drops (r=-0.760); with reviews since January 2018 the correlation drops further (r=-0.569). This is expected because the latter two, with less data, are more “noisy”. The number of TripAdvisor reviews for a property is moderately correlated with rank (r=-0.60). The TripAdvisor ranking correlates reasonably well with number of “good” review scores of 4 and 5 (r=-0.705) but poorly with the number of “bad” review scores of 1 and 2, (r=0.117). This indicates that (1) although TripAdvisor may have 50 filters as mentioned in [4], reviews remain the primary factor in TripAdvisor rankings, (2) boosting affects TripAdvisor rankings more than vandalism, and (3) a short-term boosting and vandalism campaign can influence rankings but only to some extent –the effect remains limited for properties that have a strong foundation of reviews. This pattern was observed across all 12 markets examined in our study.
We wish to examine how TripAdvisor ratings and rankings change over time. We focus on three closely-related areas of interest in TripAdvisor reviews: (1) a rapid change in the hotel ranking, either a sharp increase then a sharp decrease or vice versa, (2) sudden changes in the number of reviews submitted, and (3) an increase in volatility in the hotel rankings (i.e., several ratings of 1 or 2 interspersed with ratings of 4 and 5). For our first and second areas of interest, one challenge is getting the right window of time to evaluate – choose too small or too big of a window and some broader effects become hidden. Therefore, we make multiple passes through our data examining the changes in time windows of different sizes.
A rapid change in the hotel’s ranking, particularly a rise, indicates boosting may be at play A sudden drop is equally concerning because of a potential vandalism attack. As indicated in [17], rapid changes in the number of reviews over a short period of time may indicate burstiness. While many hotels may have special events or business cycles where more hotel stays (and more reviews) are natural phenomena, unexplainable increases are factors in boosting or vandalism campaigns. Volatility – even with no gain in the rankings – may indicate a campaign of boosting in response to poor ratings or vandalism. In each of these areas of interest, we examine these actions on TripAdvisor and look for corresponding actions in the reviews for the same hotels on Agoda/Booking.com. In our analysis, we found 34 hotels
246
(~1%) that had “bursty” behavior in TripAdvisor that was not reflected in seasonal travel, Agoda and Booking.com reviews, or in event-related details provided in the reviews themselves.
A high review frequency over an extended period on TripAdvisor without a corresponding one on Agoda or Booking.com also raises suspicions. We had 487 properties (16%) where the reviews were volatile over a smaller time window (~2 weeks to 1 month). When a larger time window (~3 months) was considered, volatility was reduced, and the reviews and ratings often resumed a more consistent pattern. However, we found 71 hotels that showed volatility over an extended time window. This may indicate volatility is a normal aspect of business or that a boosting or vandalism campaign is being performed over a longer period. This was particularly true for hotels with lower TripAdvisor ranking, which tended to have fewer reviews.
Consider two properties which have had a recent wave of positive TripAdvisor reviews by interspersed with negative reviews: the Broadmoor Hotel in Miami Beach and the Super 8 by Wyndham Chestnut Street in Colorado Springs. As of June 29, 2018, the Broadmoor Hotel has had a 8-week run of extremely positive reviews on TripAdvisor (25 reviews averaging 4.6/5) then a 6-week run of bad reviews (10 reviews averaging 2.2/5). but this is coupled with neutral-to-slightly positive ones (48 reviews averaging 6.8/10 and 49 reviews averaging 6.1/10, respectively) over the same period on Booking.com (the hotel does not appear on Agoda). The comparison with a website where stays are verified gives us confidence that these rankings are not necessarily out of place and it may be a victim of a vandalism campaign. The Super 8, on the other hand, has a series of positive reviews followed by a series of negative reviews on TripAdvisor. In contrast, it has a single (negative) review on Booking.com although it has been on the website since 2011. Unlike the Broadmoor, the reviews left by positive reviewers for the Super 8 have very few (if any) other reviews on TripAdvisor. This draws additional attention to the TripAdvisor reviews, making its volatility on TripAdvisor worthy of a second look.
B. Reviewers
1) Number of reviews
TripAdvisor gives readers of reviews the option of seeing how many reviews each contributor has made. On TripAdvisor, 19.6% of all reviewers in our 12 markets have contributed only a single review (called one-time contributors). The ratings one-time contributors give are skewed positive: 43.5% of one-time reviewers gave properties a five-star rating, versus 36.8% of all reviewers. One-time contributors are also more likely to leave a negative rating: 22.8% of the one-time reviewers left a 1- or 2-star rating as compared with 16.2% of those contributing more than one review on TripAdvisor (called frequent contributors). Unfortunately, Agoda and Booking.com do not allow an examination of the review history of its contributors.
The difference in the ratings given between one-time contributors and frequent contributors on TripAdvisor is also an important factor to identify suspicious reviews. If the divergence between ratings between the two groups is large, it
may indicate a boosting or vandalism campaign. For example, the Empress Hotel in New Orleans has 6 reviewers who gave the hotel a “low” rating (i.e., a 1 or 2) on TripAdvisor between April 21 and December 21, 2017. These reviewers made an average of 21.7 hotel reviews on TripAdvisor (including the review for the Empress Hotel). Over the same eight-month period, the Empress Hotel had 12 reviewers who gave the hotel a “high” rating (i.e., a 4 or 5). Each of these 12 reviewers made exactly one hotel review on TripAdvisor –for the Empress Hotel. On Booking.com over the same period, the Empress Hotel had 92 reviews that met our criteria and an average score of 4.4/10, with 4 of them being positive (as determined by our sentiment analysis). Agoda.com had 30 reviews, with an average score of 3.9/10, and 2 positive reviews. This difference indicated a suspicious property, particularly when the ratings differed greatly from Agoda/Booking.com over the same period. Of the hotels we reviewed, 63 hotels had a significant variance in the number of other reviews that frequent contributors left about the property as compared with one-time contributors.
Without a definitive answer on whether a hotel engaged in fake reviews on TripAdvisor or not, we can only infer that they are fraudulent – i.e., we can only flag them as suspicious. We flagged 34 properties showing “bursty” behavior, 71 hotels showing extreme ratings volatility and 63 hotels showing significant variance as suspicious and worthy of further examination. We flagged many properties for more than one reason and ended up with 103 hotels with suspicious TripAdvisor reviews in the 12 markets we evaluated.
2) Language Used in Reviews
We extracted the keywords from each of the ~5.5M reviews used in our study. Once all terms were extracted from the reviews, we eliminated stop words, tokenized, and stemmed the remaining words. This reduced the term space and allowed for better evaluation. As expected, each of our 12 markets had a different term frequency (e.g., reviewers in Miami Beach and Sydney care more about proximity to beaches and restaurants than do reviewers in Singapore and Hong Kong). What was not expected is that overall reviewers on TripAdvisor have different term frequencies than those on Agoda and Booking.com, even when we limit our comparison a single market or examine across all markets.
To compute the word distribution differences between reviewers on the 103 suspicious TripAdvisor hotels, the 2947 non-suspicious TripAdvisor hotels, and those who provided a review on Agoda/Booking.com, we use Kullback–Leibler (K- L) divergence:
D(||) = i () log2 (()/()) (2)
where () and () are the probabilities of word i in fake and non-fake reviews in each language model, respectively. The motivation behind our use of K-L divergence to detect fake reviews is the following: The Kraft–McMillan theorem [23] in information theory establishes that any coding scheme (i.e., generative model) for coding (generating) a message (a hotel review, in our context) as a sequence of random variables (terms) where each variable (term) takes one value (word/token) out of a set of possibilities (vocabulary of words).
247
The language model used for each group of reviews represents an implicit probability distribution (i.e., the underlying generative language model). Therefore, D(||) can be defined as the expected additional linguistic distributional difference that must be exhibited if a generative language model that is fitted against an incorrect distribution is used to generate reviews using language model fit on than using a generative language model fit on .
In the context of fake and non-fake reviews, D(||) provides a quantitative estimate of how much fake reviews linguistically differ (according to the frequency of word usage) from non-fake reviews. D(||), its converse, can be interpreted analogously. Consequently, we should see this quantitative estimate of divergence between the language models used in TripAdvisor reviews and Agoda/Booking.com reviews to be smaller than the divergence between the language models for real and for fake reviews.
An important property of K-L divergence is that it is asymmetric, i.e., D(||) D(||). Its symmetric extension is the Jensen-Shannon (JS) divergence:
= D(||) + 0.5* D(||) (3)
where = 0.5*(+). However, the asymmetry of K-L divergence provides us with some important information. From the definition of K-L divergence, it implies that those words which have very high probabilities in and very low probability in contribute most to K-L divergence, D(||). To examine the word-wise contribution to , we compute the word K-L divergence difference for each word, as follows:
= D (||) D (||) (4) where:
D(||) = () log2 (()/()) and D (||).
Fig. 3 shows the greatest absolute word K-L divergence differences in descending order of || for the top-k words for four datasets: (a) all TripAdvisor reviews, (b) Agoda and Booking.com reviews, (c) TripAdvisor reviewers of the 103 suspicious hotels, and (d) TripAdvisor reviews for the non- suspicious hotels. Positive values of appear above the x-axis and negative values of appear below the x-axis. We report the contribution of the top-k words to D(||), D(||), and (for k=200) for our four datasets in Table 1.
TABLE I.
Dataset
TripAdvisor (all) Booking/Agoda TripAdvisor suspicious TripAdvisor non-suspicious
K-L DIVERGENCE FOR OUR FOUR DATASETS

Unique Terms 72183 66129 49395
58184
DKL(F|N) DKL(N|F)
1.796 0.838 1.603 0.541 1.621 1.446
1.738 0.704
ΔKL JS 0.958 0.207
1.062 0.134 0.175 0.243
1.034 0.106
       
Fig. 3 shows an approximately symmetrical distribution of for the top 200 words (i.e., the curves above and below the y-axis are equally dense) for review contributors for suspicious hotels. This tells us that among the top words, there are two sets of words: the set of words which appear more in fake (than in non-fake) reviews, and the set of words, which appear more in non-fake (than in fake) reviews. The upper and

(b) 0.04
0.03
0.02
dro 0.01
WLK2 0 0 50 100 150 200
-0.01 -0.02 -0.03
k
     
(d)
0.04 0.03 0.02 0.01
dro
WL 0 K0 50 100 150 200
   
-0.01 -0.02 -0.03 -0.04
k
   
(

(c) 0.04 0.03 0.02 0.01
dro WL 0 K0 50 100 150 200
-0.01 -0.02 -0.03 -0.04
k
Fig. 3. Distribution of ΔKLWord for (a) all TripAdvisor reviews, (b) Agoda & Booking.com reviews, (c) TripAdvisor reviews from the 103 hotels we flagged as suspicious, (d) TripAdvisor reviews from the 2947 hotels we did not flag as suspicious, using unigrams and k=200.

a)
0.04 0.03 0.02 0.01
0 50 100 150 200 -0.01
dro WLK 0
-0.02 -0.03 -0.04
k
248
lower curves are equally dense. This implies that these two language models are approximately equal. The top k=200 words only contribute just above 20% to for the suspicious hotel reviews. This reveals that there are many more words in the suspicious hotel review data that have higher probabilities in fake reviews than in non-fake reviews, while also many more other words which have higher probabilities in non-fake than in fake reviews. Therefore, for the suspicious TripAdvisor review data, the fake and non-fake reviews consist of words with very different frequencies. This may indicate that those writing fake reviews may be doing a poor job at writing reviews that are believable either because they had little knowledge about the hotels they reviewed or they used a common template for writing fake reviews.
For the non-suspicious TripAdvisor review and Agoda/Booking.com review data, the top k=200 words contribute 65-70% to fake review writers use specific words in their deception that differ from truthful review writers. In contrast, for the reviewers of suspicious hotels on TripAdvisor, we find very little contribution (21.3%) of those top-k words towards . Thus, there are a number of other words in those reviews that contribute to resulting in a very different set of most-frequent words in fake reviews and in the non-fake reviews. This explains the reason for higher JS for suspicious hotel reviewers in Table 1.
explained earlier, fake and non-fake reviews use very different word distributions resulting in a higher JS.
Sussin and Thompson discovered that the flagged restaurant reviews on Yelp tended to have more extreme sentiment (either positive or negative) than other reviews. We examined the sentiment on reviews for the 103 hotels we flagged on TripAdvisor, dividing into two groups: one-time contributors and more frequent contributors (making more than 1 contribution). Our sentiment score is on a scale of (0,1) where numbers greater than 0.5 indicate positive sentiment. For those one-time contributors leaving a “good” review (i.e., reviews accompanied by a review score of 4 or 5) on the flagged hotels, the average sentiment score for each review was 0.863 versus 0.767 for all “good” reviews made by multiple contributors. For those one-time contributors leaving a “bad” review (i.e., reviews accompanied by a review score of 1 or 2) on the flagged hotels was 0.142 versus 0.189 for “bad” reviews made by multiple contributors. These results are significant (p< 0.001) and back up the findings of Sussin and Thompson.
From [13], we know the word probabilities in fake and non-fake reviews are approximately the same, i.e., () (). This results in log2(()/()) log2(()/() 0 and (||) (||) 0. Word probabilities in suspicious hotel reviews and non-suspicious hotel reviews are both very small, i.e., () () 0, resulting in similarly small values for (||) 0, (||) 0 and, consequently, 0. These conditions – and the fact that the top words contribute a large part of for reviewers for non-suspicious hotels (Table 2) – indicates that with this group of reviewers, most words in fake and non-fake reviews have nearly identical (but low) frequencies and some words contribute more to than others, and they appear in fake reviews with higher frequencies than in non-fake reviews.
Examining the one-time contributors and more frequent contributors at a higher level, we observe a further division. For TripAdvisor reviewers who have written other reviews on TripAdvisor, we find that they focus more on tangible room details (e.g., “the bed was hard”, “the towels were threadbare”) whereas one-time reviewers focus more on intangibles that are easy for others to understand but difficult for others to corroborate, like “poor service” or “the room was depressing”. Often reviewers who we flagged as suspicious focused on details that other guests could not easily validate (e.g., “great service”), or focused on defending the property from other reviews (e.g., “I don’t know what others are talking about!”). Some used extreme superlatives, e.g., “the best stay ever” or “I have never been so comfortable!”. Most TripAdvisor reviewers which contributed more than one review and most Agoda and Booking.com reviewers used this extreme form language more sparingly. This division was seen across all 12 markets.
Backing up the findings of fake reviews by Burgoon et al., we found that contributors of TripAdvisor reviews on suspicious hotels used simpler, short, and fewer average syllables per word than either non-suspicious TripAdvisor hotel reviewers or Agoda/Booking.com reviewers. Table 3 illustrates these findings for each of our 4 datasets.
TABLE II.
Dataset
TripAdvisor (all) Booking/Agoda TripAdvisor suspicious TripAdvisor non-suspicious
PERCENTAGE CONTRIBUTION TO DIVERGENCE FOR TOP-200 WORDS IN EACH DATASET

DKL(F|N) DKL(N|F) ΔKL 70.4 15.1 62.4
74.5 15.7 65.5 29.8 8.6 21.3 72.2 16.1 64.5
       
From Table 2, we find that the top-k words contribute 70- 75% to DKL(F||N). This indicates reviewers of suspicious hotels use specific words related to deception more frequently than those non-suspicious hotel reviewers. These words are responsible for the deviation of the word distribution in reviews from the 103 flagged hotels from the unflagged set of 2947 hotels. However, for DKL(N||F), we see a very small contribution of those top words in the data from the non- suspicious hotel reviewers This reveals that a majority of the non-suspicious hotel reviews do not purposely use specific words. This makes sense – to write about a genuine experience, most reviewers who stay at a property do not focus on using a small set of words but instead describe their experience from a more universal set of words.
We also examined bigram distributions and examined different values for k (k=300, k=500), which yielded similar trends for D and However, we found smaller values for D and higher values for because bigrams add sparsity to the term space – the net probability mass is distributed across more terms. The symmetric Jensen–Shannon (JS) divergence results show that the values for the non-suspicious TripAdvisor hotel reviewers are less than half of that for the TripAdvisor reviewers submitting reviews for suspicious hotels. As
249
TABLE III. AVG READABILITY (GUNNING FOG SCORE), LENGTH IN WORDS, AND NUMBER OF SYLLABLES/WORD USED IN EACH DATASET
VI. CONCLUSIONS, LIMITATIONS AND FUTURE WORK
Using analytical techniques to detect language and activity patterns, we have examined signals of potential fraud in hotel reviews, reviewers and hotels using more than 5.5 million hotel reviews from 3050 hotels in 12 markets – one of the largest studies of hotel review fraud to date. Our research compared two types of contributions: TripAdvisor, where no verification of an actual hotel stay is made before a review is posted and Agoda and Booking.com, where only reviewers who booked through their website can review properties. Although the ratings posted on TripAdvisor and Agoda/Booking.com are highly correlated, there are ways in which TripAdvisor ratings can be manipulated, either by a campaign of providing good reviews for a property (boosting) or by providing bad reviews for a competing property (vandalism).
Using a longitudinal evaluation of TripAdvisor rankings, we examined hotels that had a rapid change in their ranking, either a sharp increase then a sharp decrease or vice versa, had sudden changes in the number of reviews submitted, or an increase in volatility in the hotel rankings (i.e., several ratings of 1 or 2 interspersed with ratings of 4 and 5). Across the 3050 hotels in 12 markets, we uncovered 103 hotels that displayed signs of review fraud. Looking into these properties further, we find that independent hotels and hotel chains comprised primarily of franchisees are more likely to display signs of review fraud than corporate-run chains. Properties that have had a large number of reviewers who contributed only a single TripAdvisor review are more likely to leave reviews that are shorter and use fewer words.
We also examined the language and sentiment used in the reviews themselves. TripAdvisor reviews contained different term frequency and different sentiment than Agoda/Booking.com reviews. When TripAdvisor reviewers of our 103 suspected properties are examined separately, the language used was markedly different from reviewers who left TripAdvisor reviews on properties we did not find suspicious.
There are several limitations of our study. First, we examined reviews only in English and only in 12 markets. Second, we cannot verify that the suspected properties engaged in review fraud, so we can only infer fraudulent intent. Third, even among hotels with suspicious reviews, there were many genuine contributions that we did not separate from suspicious ones in our study. Fourth, although we examined one-time contributors separately from multiple contributors, we acknowledge that every contributor started as a one-time contributor and being a single contributor certainly does not imply wrongdoing.
In future work we plan to examine the language used in separate markets and how sentiment is affected by regional differences in culture, which will allow us to build a more robust fraud language filter. We will also look at more sophisticated review fraud; for example, how boosting and vandalism efforts may go together as a single campaign. Last, we plan to expand our work to examine how review aggregators like Trivago are affected by review spam.

Dataset
TripAdvisor (all) Booking/Agoda TripAdvisor suspicious TripAdvisor non-suspicious
C. Hotels
Readability Length
9.7 74 9.3 66 8.2 51 9.8 76
# Syllables
2.36 2.61 2.05 2.41
       
In their evaluation of restaurant review fraud on Yelp, Sussin and Thompson indicated that chain restaurants were less likely to commit review fraud than independents. In our 12 markets, we found that one-time reviewers of hotels on TripAdvisor were 11.2% more likely to leave reviews for independent hotels than for chain hotels (we determined chain hotels by an affiliation of a branded chain in their property name). In our data, independent hotels made up just over 37% of hotels, which is far greater than the 17% Mayzlin et.al. found in the hotels used their study. This difference is likely an artefact of the markets we explored (Mayzlin et.al. used data from US markets only, which has a greater percentage of chain-affiliated hotels). In the 103 hotels we flagged as suspicious, 63 (61%) were associated with independent hotels.
Mayzlin et.al. mentioned that hotels associated with chains are less likely than independent hotels to engage in boosting or vandalism; however, one issue they didn’t examine was the difference between chain hotels operated by franchisees and chain hotels operated by a corporation. Wyndham hotels, one of the largest hotel brands with 8,400 hotels, operates under the franchise model (100% of Wyndham hotels are franchised) whereas others such as Hilton and Marriott have far fewer franchisees (41% and 43% of their properties are franchised, respectively). Moreover, many Wyndham franchisees operate a single hotel whereas Hilton and Marriott hotels are frequently operated by groups that operate multiple properties. Therefore, some chains act more like independent hotels, and some hotel managers – even those operating under a brand – have more to lose from bad reviews than others. Franchises are more common in hotels than restaurants. When we consider hotel chains that are largely comprised of independent franchisees, such as Accor, Choice, Red Lion, and Wyndham, the percentage of hotels with suspicious reviews increases from 61% to 88%, despite having fewer reviews per property than other chains.
In their evaluation of review fraud, Sussin and Thompson found that restaurants with a smaller base of established reviews or that received a recent batch of bad reviews were more likely to commit review fraud. Having a larger base of TripAdvisor reviews can mitigate the effects of hotel review fraud; hotels with more reviews are more likely to be the chains with few franchisees and are likely to be ranked more highly on TripAdvisor, Additionally, more highly-ranked hotels are less likely to engage in review fraud. Of the 103 hotels we flagged as suspicious, 65 of the 87 hotels (75%) we flagged for potentially engaging in boosting did so after a sustained period of bad reviews and subsequent decrease in their TripAdvisor ranking. We did not find a similar pattern for the 16 hotels we flagged as victims of vandalism.


Insights into Suspicious Online Ratings: Direct Evidence from TripAdvisor
Markus Schuckert1* , Xianwei Liu2 and Rob Law1 1School of Hotel & Tourism Management, The Hong Kong Polytechnic University, 17 Science Museum Road, TST-East, Kowloon, Hong Kong SAR 2School of Management, Harbin Institute of Technology, 92 Xidazhi Street, Harbin, 150001, People’s Republic of China
Online ratings and online reputation management are becoming increasingly popular and important. With this increasing importance, attempts to manipulate online reviews through fake reviews have become more prevalent. Suspicious online reviews (ratings) exist on many e-commerce platforms, but these reviews have rarely been observed and reported as manipulation in academic studies using different test methods. In our research, we examine empirical evidence of suspicious online ratings based on 41,572 ratings on Tri- pAdvisor. Applying quantitative analytics, we find three important results: (1) the gap between overall rating and individual ratings does exist and is significant, especially among the lower class hotels; (2) the proportion of suspicious ratings is about 20% at a standard of 0.5; and (3) reviewers who tend to post excellent ratings are less likely to gen- erate big gaps when posting ratings. We offer specific managerial implications for hotel managers on online reputation management and selected suggestions for future research based on the empirical findings.
Key words: suspicious ratings, social media, reputation management, TripAdvisor
Introduction
With the increasing popularity of e-commerce platforms and online social media activities, every industry is now involved in the online sale of products and/or services and the collec- tion of feedback on them. The hospitality and
*Email: markus.schuckert@polyu.edu.hk © 2015 Asia Pacific Tourism Association
tourism industry is no exception. Today, all categories of hotels employ online travel agents (OTAs) or booking platforms to diver- sify their sales channels and reach out to more potential customers. The volume of online sales is increasing steadily across all sectors; especially for airlines and hotels,

2 Markus Schuckert et al.
online sales have become the biggest part of their revenue (Buhalis & Law, 2008).
After returning home from their travels, cus- tomers want to give feedback online them- selves or are asked to do so by the hotel, the OTA, or the booking platform. Such online reviews and ratings given in them have become increasingly important for the compa- nies and employees involved. From the per- spective of potential customers, these reviews are considered to be authentic, helpful, and influential (Li & Hitt, 2008). From the per- spective of the company and its employees, online reviews are a fast, instant, and easy accessible customer feedback and have become word of mouth on, and the carrier of, a company’s reputation in the digital age (Kaplan & Haenlein, 2010). Thus, online reviews play a critical role in the online sales of the hospitality and tourism industry, which mainly offers services and focuses on customer satisfaction.
The reviews on e-commerce platforms profit both travel agencies and travelers. On the one hand, e-commerce makes it easier for custo- mers to purchase tourism products or services on the basis of recommendations and electronic word of mouth; on the other hand, customers’ online buying can significantly increase travel revenue (Shaw, Bailey, & Williams, 2011; Zhang & Mao, 2012; Zhu & Zhang, 2010). The online platforms gener- ate feedback through numerous online reviews and ratings, upon which potential online customers base their purchase decisions. It has been found that buyers always follow and prefer the high-rated hotels or restaurants (Ye, Law, Gu, & Chen, 2011). Potential custo- mers want to find the right place and be sure about it, and this is the reason why they spend so much time reading online reviews to support their decision-making (Zhu & Zhang, 2010). Customers like to search for
objective, true, or authentic opinions, and they prefer reviews, which are mostly delivered through large feedback platforms and consu- mer-centric sites, because of their assumed independence from official or corporate content (Forman, Ghose, & Wiesenfeld, 2008).
The proliferation of e-commerce and the increasing number of product (service) reviews generated by users have changed the lifestyle of individuals. Previously, purchase decisions were based on offline or online advertisements and other product information provided by sellers. However, nowadays, con- sumers and businesses increasingly trust and rely on online reviews in their search for infor- mation to make purchase decisions if the ratings are truly from other customers (Mauri & Minazzi, 2013; Yacouel & Fleischer, 2012).
But are online reviews (ratings) reliable enough for making choices? What happens if consumers and/or companies abuse online ratings and feedback platforms to manipulate ratings or submit false ratings for various reasons? Studies have examined the reliability, credibility, and helpfulness of online reviews from the online readers’ angle (Bissell, 2012; Mkono, 2012; Xiang & Gretzel, 2010; Yoo & Gretzel, 2010). Up to now, how to deter- mine or detect suspicious online reviews (for instance, fake, manipulated, or irresponsible reviews) remains a challenging question (Hu, Bose, Koh, & Liu, 2012). Although every e- commerce platform has many rules to ensure the authenticity of every review posted by cus- tomers, review manipulation is not a hypothe- tical phenomenon (Hu et al., 2012).
In our research, we suggest an objective method for ascertaining whether an online review posted on a particular e-commerce plat- form (TripAdvisor) is unauthentic or suspi- cious, which is very important for managers who have their hotels or restaurants on this influential platform (Lee, Law, & Murphy,
2011). Although this method is a shortcut and in practice has limited universal generalizability to other e-commerce platforms due to differ- ences in web design, we obtain some valuable and unexpected findings based on the biggest and most influential OTA. Through the method and related findings based on a specific review platform, we may be able to generalize the use of this method to other OTAs.
In this paper, we set off to discover the pres- ence of suspicious reviews (ratings), to identify the proportion of these reviews, and to explore who tends to post this kind of review. More specifically, we address the following research questions:
		(1)  Are some online ratings self-contradictory due to lying or perfunctoriness? 
		(2)  If contradictory online ratings do exist, how large is the proportion of such reviews and how can the most suspicious ratings be identified? 
We begin by providing insights into the current state and limitations of the recent literature. Then, we describe our research design and give information about the data sampling and the data collection process. The results of our study are then presented and the findings dis- cussed. In the final part of the paper, we discuss the implications of these findings for users, industry, and researchers as well as future research opportunities. The paper con- cludes with a critical view of the limitations of this study.
Background
Due to the experiential nature of tourism pro- ducts and services, online reviews are more important in the hospitality industry since
many potential trip planners often rely on this source of information when making decisions (Litvin, Goldsmith, & Pan, 2008; Yoo & Gretzel, 2011). Thus the websites that provide reviews are becoming hubs for poten- tial travelers (Cox, Burgess, Sellitto, & Buult- jens, 2009; Liu, Schuckert, & Law, 2014; Schuckert, Liu, & Law, 2014). As consumers are increasingly depending on information released through social online channels, especially online reviews (ratings), to make product or service purchase decisions, the quality and truthfulness of the information available to them are important. Given the important role of reviews in travel information searches and decision-making (Yoo & Gretzel, 2008), there is a definite need to judge the reliability of online reviews. Studies have revealed that online reviews are not that reliable nor did they offer methods and pro- cedures to solve this issue. Mack, Blose, and Pan (2008) noted that the information quality and the expertise of these sources vary tremen- dously. Racherla, Connolly, and Christodouli- dou (2013) found that the correlation between overall review rating and ratings on individual attributes is very low, suggesting that the overall numerical ratings typically used in review systems may not be the ideal indicators of customers’ perceived service quality and sat- isfaction. Furthermore, Hu et al. (2012) dis- covered that around 10.3% of products are subject to online review manipulation.
This evidence suggests the need to take a closer look at online reviews (ratings) in general and in particular at those that seem to be suspicious. The most concerning aspect of suspicious or poor-quality reviews is manipulation, defined by Hu et al. (2012) as the consistent monitoring of online reviews and the posting, when needed, of non-auth- entic customer online reviews by vendors, pub- lishers, writers, or any third party with the goal
Insights into Suspicious Online Ratings 3
4 Markus Schuckert et al.
of boosting sales of their products. Figure 1 shows an example of manipulated reviews in which we noticed the suspicious behavior of a customer who frequently posted positive reviews; this user had visited the website every few days to post reviews with different textual comments giving very high ratings for the same product.
Although every e-commerce platform has a system and procedure to ensure the authenticity of every review posted by customers, review manipulation is not a hypothetical phenomenon (Hu et al., 2012). It is known to exist widely on popular websites related to e-commerce, travel, music, and so on: For example, Amazon’s Cana- dian website has revealed a sizable proportion of fake reviews of some books which are posted by the publishers and authors of the books and their friends or relatives to increase online sales (Gurun & Butler, 2012). Review manipulation is not just prevalent among online book sellers. The music industry is known to hire professional marketers to surf various online chat rooms and fan sites and post positive comments about new albums in order to promote sales (Mayzlin, 2006; White, 1999). It also exists in the hospitality industry (e.g. hotels and restaurants). Positive reviews and ratings can be posted by an owner and his/her friends in order to gain a high rating, attract potential buyers, and boost sales, while negative reviews (ratings) are used to defame competitors (Hu et al., 2012).
Consumers may make wrong purchase decisions based on manipulated online reviews (ratings). However, to date, few studies have investigated and reported the pres- ence of manipulated reviews. To the best of our knowledge, there are only three recent research papers that have focused on proving the exist- ence of online review manipulation (Hu, Bose, Gao, & Liu, 2011; Hu et al., 2012; Hu, Liu, & Sambamurthy, 2011). However, although
current works have offered ways to identify products or services whose reviews are manipulated, the validity of their methods cannot be guaranteed. For example, detecting fake reviews by the writing style (sentiments and readability) assumes that the writing styles of manipulators will be different from those of genuine consumers, and detecting them by the fluctuations in mean consumer ratings over time involves a complex compu- tational process. However, as is known, if a method is based on assumptions or involves a complex computational process, the results will be further away from the facts.
Here, we use a direct approach (without a complex counting process or an assumption) to prove the existence of suspicious online ratings. Our research is also the first attempt to conduct a quantitative study of suspicious online reviews in the hotel industry. The three related studies mentioned above emphasized the manipulation of positive ratings that can increase the online sales of the sellers rather than the negative ratings (vicious competition within the industry). Furthermore, suspicious online ratings include not only manipulation which is a purposive behavior but also purpo- seless behavior; perfunctory ratings may also widely exist. In our research, we do not want to draw a distinction between different types of suspicious ratings since any subjective judg- ment is meaningless. Our purpose is to provide direct evidence of suspicious online ratings and the proportion of these ratings as well as the rating behavior in the hotel industry.
Methodology
Data and Variables
TripAdvisor allows reviewers to give two kinds of ratings at one time: an overall rating and
specific ratings. Interestingly, the overall rating is not computed by the average of the specific ratings: they are independent. This design of TripAdvisor is quite different from many other e-commerce platforms such as Ctrip, Agoda, and eLong (these platforms only allow customers to give specific ratings: the overall rating is computed from the specific ratings), and this makes it possible for us to check whether some reviewers are lying using quantitative analysis. However, it is not only TripAdvisor that allows customers to give two kinds of ratings: FlipKey.com also has the same design. Thus, our method can be
Figure 2 Overall Rating.
applied on platforms with the same web design.
Figure 2 below shows the rating system for TripAdvisor members to post online reviews and ratings: members can give an overall rating for a property; also, they have the option to give specific ratings for service, value, sleep quality, cleanliness, location, and rooms (see Figure 3). A crawler was developed to retrieve both kinds of ratings from TripAdvisor (Ye, Law, & Gu, 2009), and focus on every star-rated hotel in Hong Kong listed on TripAdvisor. We also acquired web pages of each reviewer’s infor-
Insights into Suspicious Online Ratings 5

Figure 1 Example of Manipulated Reviews. Source: Hu et al. (2012, p. 675).

Source: TripAdvisor.
6 Markus Schuckert et al.

Figure 3
Specific Ratings. Source: TripAdvisor.
Table 1 presents the variables used in the fol- lowing analysis. Customers can post two kinds of ratings at one time: overall rating and specific ratings. We first average the specific ratings of every post to compute the variable “Average”, and then calculate the difference (gap) between “Overall” and “Average”. The rating distribution of every customer is also calculated, for instance, if a customer posted 5 ratings including 1 terrible rating, 2 poor rating, 2 excellent rating, then the terrible rate will be 20%, poor rate 40%, excellent rate 40%.
Suspicious Examples and Research Design
We used the gap between the overall rating and the average rating as the index to measure whether a review or a rating is suspicious (fake or perfunctory). Below are two examples of suspicious ratings to illustrate our research design. As can be seen, the overall rating of the review in Figure 4 is 5, but the specific ratings diverge significantly from this score (Value: 1, Location: 3, Sleep: 1, Rooms: 1,
mation (the distribution of every reviewer’s ratings). In addition, we developed another Java-based program to parse HTML and XML web pages into our database. Data col- lection was conducted in August 2013, and the crawler was used to retrieve all available ratings of TripAdvisor-listed and star-rated hotels (185 hotels in total, including 18 five-star hotels, 80 four-star hotels, 75 three-star hotels, 11 two-star hotels, and 1 one-star hotel) in Hong Kong. The complete rating data were collected from each hotel since the moment the hotel joined TripAdvi- sor. Records lacking information or repeating records were deleted, resulting in a research sample total of 41,572 valid ratings.

Variable
Overall Average Gap ABS(Gap) Excellent rate Very good rate Average rate Poor rate Terrible rate Hotel class
Table 1 Variables Used for Analysis Description
The customer’s overall rating of a property Average (value, location, sleep, rooms, cleanliness, service) Gap = Overall−Average The absolute value of Gap The number of excellent ratings of a reviewer/the total number of his/her ratings The number of very good ratings of a reviewer/the total number of his/her ratings The number of average ratings of a reviewer/the total number of his/her ratings The number of poor ratings of a reviewer/the total number of his/her ratings The number of terrible ratings of a reviewer/the total number of his/her ratings Class according to TripAdvisor (from one star to five star)
 
Figure 4 Suspicious Ratings for Hotel A. Source: TripAdvisor.
Cleanliness: 1, Service: 1; Average: 1.33), the Gap being 3.67; the example given in Figure 5 is similar. If we check the content of these two reviews, we find that the specific ratings rather than the overall rating in Figure 4 are more likely to be the reviewer’s real evaluation, while in Figure 5, the overall rating seems to be the true evaluation of the reviewer rather than the specific ratings. The self-contradiction between overall ratings and specific ratings stimulated us to explore whether the given examples are individual cases or exist exten- sively and what causes this phenomenon.
In order to explore the difference between overall ratings and average ratings as well as the specific ratings, we ran a paired sample test and a regression analysis, and we further checked the suspicious review rate on the basis of different standards.
Furthermore, in order to explore what kind of reviewers (hotels) tend to post (get) self- contradictory ratings, we ran a correlation
analysis between the gap and the distribution of reviews (Excellent, Very good, Average, Poor, Terrible) of each reviewer as well as hotel class.
Results and Discussion
Descriptive Statistics
The average value of overall rating among all 41,572 ratings was around 4.2, which suggests that travelers are very satisfied with their experience in Hong Kong hotels in general. The absolute value of the gap ranged from 0 to 4, the average value being 0.336. Reviewers can give an overall rating from 1 to 5: According to the descriptive statistics, most reviewers (76.4%) tend to give 5 and 4 ratings, suggesting that most travelers feel satisfied with the hotels or restaurants they have experienced. However, 15.5% of the tra-
Insights into Suspicious Online Ratings 7

8 Markus Schuckert et al.

Figure 5 Suspicious Ratings for Hotel B. Source: TripAdvisor.
velers in our sample thought that their experi- ence was just so-so, and only 8.1% regarded their satisfaction level as poor or terrible (Table 2).
Gap Detection and Evaluation
We first ran a regression analysis to examine the explanation degree of specific ratings on overall rating since if the two kinds of ratings are consistent, the adjusted R2 should be near 100%. Table 3 shows the regression results. The adjusted R2 is 74.5%, which has quite a big difference from 100% (Racherla et al., 2013); thus, the gap does exist. The gap
between overall rating and specific ratings indi- cates the “evaluation difference” of hotel guests. In addition, we found that the most important specific rating is Service (the coeffi- cient is 0.265), followed by Rooms, and that the least important specific rating is Location. So the hotel industry should pay more atten- tion to service and rooms since the guests care about these more. Service is the key factor in the hospitality industry; guests usually put more weight on the service quality of a hotel. Room space is part of the hardware of a hotel and is the most important attribute of accommodation.
We then made a comparison between the overall rating and the average ratings to
Table 2
Insights into Suspicious Online Ratings 9 Descriptive Statistics

N Min 41,572 1
41,572 1 41,572 0 41,572 0 41,572 0 41,572 0 41,572 0 41,572 0
Max
5 5 4 1 1 1 1 1
Mean
4.155 4.194 0.336 0.400 0.364 0.155 0.051 0.030
S.D.
0.894 0.730 0.320 0.233 0.194 0.132 0.078 0.069
Sig.
0.000 0.000 0.000 0.000 0.000 0.000 0.000

Overall Average ABS (Gap) Excellent rate Very Good rate Average rate Poor rate Terrible rate
Model Constant −0.064***

Table 3
Regression Analysis Unstandardized coefficients
 
Value Location Sleep quality Rooms Cleanliness Service F (p-value) Adjusted R2 N Dependent variable
**Significant at the 0.05 level. ***Significant at the 0.01 level.
0.175*** 0.095*** 0.116*** 0.253*** 0.109*** 0.265*** 0.000
74.5%
Beta
S.E.
0.017 0.004 0.003 0.004 0.004 0.005 0.004
29,397 Overall
 
confirm this gap. Table 4 shows that overall ratings and average ratings are significantly highly correlated, the coefficient reaching 0.857, but they also have significant differ- ence at the 0.000 level, which indicates that the overall ratings are different from the
different levels
average ratings. We then set of gap standard and count the ratings which exceed the gap standard. As can be seen in Table 5, there are 13,011 ratings whose gap exceeds 0.5 (including 0.5), accounting for more than 31% of the
10 Markus Schuckert et al. Table 4

Variable
Overall Average
Paired Samples Correlation and Test Correlation (sig.) T (sig.)
0.857 (0.000) −16.927 (0.000)
Gap Standards and the Corresponding Suspicious Rates
 
Table 5

ABS (Gap) standard
N
%
≥0.5 13,011
31.30
>0.5 ≥1.0 ≥1.5 ≥2.0 8,272 2,128 329 92
19.90 5.12 0.79 0.22
Figure 6 shows the 41,572 points: each point represents a gap between a review’s overall rating and the average of its specific ratings (Value, Location, Sleep, Rooms, Cleanliness, and Service). We can see that most of the gaps
 
sample; when we use a loose value at 0.5 (not including 0.5), almost 20% of the ratings are over this gap; the corresponding figures for the gap standards of 1.0, 1.5, and 2.0 is 5.12%, 0.79%, and 0.22%, respectively.

Figure 6 Scatter Diagram.
are in the interval [−1.0, 1.0]. Conservatively, if we define the value as over 0.5, then we find that about 20% of reviews are suspicious: this is a large and considerable proportion. Numerous online ratings are generated every day by custo- mers who have experienced hotels; through our examination of the historical online ratings of Hong Kong hotels listed on TripAdvisor, we found a sizable proportion of suspicious ratings. This is an alarm bell of this e-age since travelers all over the world rely heavily on online ratings to make purchase decisions. Wrong information or unrealistic ranking of hotels can mislead consumers and result in decision failures. The suspicious ratings defined by this study can be regarded as low- quality online reviews which cannot offer useful information to readers.
Rating Behavior
In order to further explore who tends to post suspicious ratings, we ran a correlation analy- sis, the results of which are shown in Table 6. Except for the significantly negative relation- ship between Excellent rating and ABS (Gap), the other variables (Very good, Average, Poor, and Terrible ratings) are all positively
related with ABS (Gap), which indicates that reviewers who tend to give excellent ratings are less likely to create big gaps and reviewers who dislike giving excellent ratings are more likely to generate big gaps. In other words, reviewers who give high appraisals and indi- cate high satisfaction give their ratings care- fully and from the heart. Reviewers who give relatively fewer excellent ratings and prefer to give scores of 1–4 may not think carefully about the ratings they give, rather just posting them randomly without consideration and objectivity. For example, reviewers who give more of poor or terrible ratings may just want to take revenge and do not care about the reality, and those who like to give average or very good ratings may regard it as a routine. So reviewers who usually prefer to give fewer excellent ratings are more likely to be lying; generally, those who prefer to give excellent ratings are telling the truth from the heart. Another possibility is that reviewers who tend to give excellent ratings include more manipulators whose job is to constantly post positive reviews for many hotels. These manipulators are required to post both overall ratings and specific ratings of 5 by hotel managers who need high online ratings. As is known, a high online rating represents
Table 6 Correlation Analysis
Insights into Suspicious Online Ratings 11

ABS (Gap)
ABS (Gap) 1.000 Class −0.082** Excellent −0.132** Very good 0.035** Average 0.082** Poor 0.095** Terrible 0.080**
**Significant at the 0.01 level.
Class
1.000
0.133** −0.088** −0.090** −0.022** −0.003
Excellent Very good Average Poor Terrible
1.000 −0.695** 1.000 −0.539** −0.029** 1.000 −0.251** −0.163** 0.039** 1.000 −0.105** −0.226** −0.059** 0.094** 1.000
 
12 Markus Schuckert et al.
high online reputation and can attract more online buyers, which may be transferred into online sales. Customer ratings can decide the ranking of a hotel on TripAdvisor. The top ranking hotels are always shown in the top position and on the first page of a website, thus naturally gaining priority in terms of being considered by potential online buyers. Thus, online rating is a kind of wealth or competitiveness for a hotel. Given this situ- ation, hotel managers are encouraged to hire professional reviewers to generate positive reviews (ratings) for them.
The relationship between hotel class and ABS (Gap) is also explored; as the second row shows, the ratings from higher class hotels are associated with lower gaps (the coef- ficient is significantly negative), indicating that the problem of suspicious online ratings is more serious among the lower class hotels. In addition, as the second column shows, higher class hotels can get more excellent ratings and less other ratings such as very good, average, poor, and terrible ratings, suggesting the high class hotels listed on TripAdvisor are of high quality and get guests’ approval.
In our opinion, suspicious online ratings can be generated in two ways: with purpose (manipulation, also called fake reviews) and without purpose (perfunctory rating behavior). No matter which way they are created, suspi- cious ratings are unhelpful to online buyers since readers cannot get actual information on the evaluation of a hotel experience.
Conclusion
Through analyzing overall ratings and specific ratings from TripAdvisor, we find that there is a significant difference between the two kinds of ratings. The gap between the two kinds of ratings exists widely and is confirmed by our
empirical study; to be more specific, if we set the gap standard as 0.5, the proportion of suspicious online ratings is 20%. Furthermore, reviewers who tend to give more excellent ratings are less likely to generate big gaps when posting ratings. In other words, they give their high evaluations more carefully and sincerely, or, more likely, they are manipula- tors hired to post positive ratings. Further- more, the issue of suspicious online ratings is more serious among the lower class hotels.
Our research has many practical appli- cations. First, we suggest that TripAdvisor should offer an automatic warning function when a reviewer posts a rating with a gap of over 0.5 (or some other value) since he/she may have made a mistake or may not be taking the rating seriously. Since self-contra- dictory ratings may confuse readers and cause unnecessary waste of time, the existence of this kind of ratings makes no sense. Since e-commerce platforms are similar, our findings from TripAdvisor should remind other e-commerce platforms about the problem of fake or low-quality reviews. In addition, by checking the proportion of suspi- cious ratings of every hotel (restaurant) listed on TripAdvisor, it is possible to determine whether there has been manipulation. Second, this study offers a timely alert for online buyers who seriously depend on online reviews when making purchase decisions. We suggest that readers pay more attention to the rating gap when searching for useful infor- mation to help them make decisions because some reviews may be fake and therefore are not based on true facts. According to our find- ings, perhaps 20% of reviews are not posted carefully.
Our research also has limitations. First, the method employed in this research to ascertain suspicious reviews cannot be applied on most e-commerce platforms due to differences in
web design, so future research should do more in this field. Second, although we have com- puted the proportion of suspicious ratings on the basis of different gap standards, we cannot confirm which standard is the reality: In other words, how big the gap should be before we can regard a review as a low- quality review is still unknown. Third, we did not include the personal characteristics of reviewers in our analysis, even though individ- ual attributes can significantly affect online be- havior. Gender, age, and other personal information are not part of our dataset because this information is private and not dis- closed by TripAdvisor. Future studies might usefully consider these factors by using the dataset from other OTA platforms when exploring reviewer behavior. Fourth, TripAd- visor operates in 34 countries worldwide and has more than 260 million monthly visitors as well as over 125 million reviews; thus the reviewer nationalities in our dataset range extensively. Hence it is not manageable and appropriate to geographically divide all the nationalities into two or more groups to analyze the difference among reviewers from different countries. Lastly, this is also a disad- vantage of using big samples collected from websites rather than using questionnaires which can be well controlled.

Contrasting Fake Reviews in TripAdvisor (discussion paper)
Francesco Buccafurri1, Michela Fazzolari2, Gianluca Lax1, and Marinella Petrocchi2
1 DIIES, University Mediterranea of Reggio Calabria Via Graziella, Localit`a Feo di Vito 89122 Reggio Calabria, Italy E-mail:{bucca,lax}@unirc.it
2
Istituto di Informatica e Telematica - CNR Via G. Moruzzi, 1
56124 Pisa, Italy E-mail:{m.fazzolari,m.petrocchi}@iit.cnr.it
Abstract. Fake reviews are a concrete problem still affecting the relia- bility of systems like TripAdvisor, especially in the case of few reviews, thus in the first, most vulnerable, activity period of operators. In this work-in-progress paper, we present a model aimed to contrast this prob- lem, based on a sort of normalization of scores given by users, to take into account the level of assurance of the reviews. This is done by consid- ering two different dimensions, combined each other, which are the level of assurance of the identity of the review’s author and the level of assur- ance of the occurrence of the evaluated experience. The paper presents a first validation of the approach conducted on real-life data, giving us very encouraging results.
Keywords reputation model, trust management, TripAdvisor. 1 Introduction
Over the last years, online reviews became very important since they reflect the customers’ experience about a product or a service and nowadays constitute the basis on which the reputation of an organization is built. Online reviews have a great influence on the purchase decisions of other customers, who are increasingly relying on them [6].
Unfortunately, the confidence in such reviews is often misplaced, due to the fact that scammers are tempted to write fake information in exchange for some reward or to mislead consumers for obtaining business advantages [8]. These reviews are called opinion spam or fake reviews [4, 5].
SEBD 2018, June 24-27, 2018, Castellaneta Marina, Italy. Copyright held by the author(s).
The identification of fake reviews is not an easy task, since they can be identical to genuine ones. Nevertheless, several automatic techniques have been proposed in recent years. Fake reviews detection involves the identification of a set of features, linked with the content (review centric features) or with the review author (reviewer centric features).
Most of the existing machine learning approaches are not sufficiently effective in spotting fake reviews, nevertheless they are more reliable than manual detec- tion. There exist several studies in the literature that rely on machine learning approaches and consider different set of review features [7,10]. Further stud- ies consider also reviewer centric features [9], which cannot be extracted from the text of a single review. In addition, graph-theory based approaches have been investigated to find relationships between reviews and their corresponding authors [3]. The spam detection techniques that combine reviews features and reviewers behaviors normally lead to better results [11].
In this work-in-progress paper we propose an approach different from the previous fake review detection approaches. Moreover, we introduce a reputa- tion model and its preliminary experimental validation, designed to contrast the phenomenon of fake reviews in TripAdvisor. TripAdvisor is a very famous travel Website collecting reviews of travel-related contents. On the basis of these re- views, an aggregate score of each content is shown. Due to the economic value related to the effects of this system, and despite the efforts declared by TripAd- visor, the system is not immune from the problem of dishonest reviews, aimed either to fictitiously promote a given operator (i.e., self promoting attack) or to denigrate a competitor (i.e., slandering attack). Therefore, the noise occurring in the reviews derives not only from physiological subjectivity of the users [1]. Starting from a preliminary proposal [2], we define a model and a consequent methodology aimed to (partially) purify the system from the noise coming from fake reviews, in order to obtain more reliable normalized scores. This is done by considering two different dimensions, combined each other, which are the level of assurance of the identity of the review’s author and the level of assurance of the occurrence of the evaluated experience.
It is worth noting that the approach here proposed is heuristic and any feature used to compute the trust is based on reasonable argumentations and ad hoc observations of the phenomenon, with no a specific validation of any single feature. Anyway, the paper is just aimed to give a general experimental validation of the whole approach and, thus, of the global combination of any feature.
The novel contributions, w.r.t. the initial proposal presented in [2], is that the model does not introduces new features required to TripAdvisor (in favor of the practical relevance of the proposal) and that this paper includes also an experimental validation of real-life TripAdvisor data.
The structure of the paper is the following. In the next section, we define the proposed reputation model. In Section 3, we describe the experiments carried out to validate our proposal. Finally, our conclusions are summarized in Section 4.
2 The Reputation Model
In this section, we describe our reputation model, whose components are listed in the following.
	1	A set U of users, corresponding to the set of travelers, potentially covering all Web users. 
	2	A set S of service providers, corresponding to the set of restaurants, bars, hotels, and other operators registered to TripAdvisor. 
	3	For each service provider s ∈ S, a list of feedbacks R(s) (which are the reviews), each corresponding to a transaction. A feedback rs ∈ R(S) for the service provider s is a tuple ⟨u, d, v, k, t, I⟩, where u is the author of the feedback, d is the time of the feedback, v is the time of the transaction, k is the score given by u on s (it is the aggregate score – also detailed into different dimensions), t is a text motivating k (it is the text included by the user to describe the experience) and I is the set of additional resources (it is the images posted by the user). Concerning u, we rely on the set of attributes which can be drawn from the system (and also by using external sources, like social-network sources).  The transaction corresponding to a feedback rs is denoted by tr. To imple- 
ment the notion of certified reputation in this model, for a given feedback rs, we need two basic measures, which are both numbers ranging in the interval [0, 1]: (1) the trustworthiness of the identity of u, denoted by trust(u), and (2) the trustworthiness of the transaction tr, denoted by trust(tr).
The first measure trust(u) is related to identity management issues and is a measure of the level of assurance of identity proofing given by the registration phase into the reputation system. Observe that we do not consider the level of assurance of the authentication phase, thus assuming that no attacks on accounts of users occur. This measure may take into account also external information associated with the digital identity of the user in the system (e.g., information coming from different sources as online social networks). We remark that the trustworthiness of the identity of u is directly related to misbehaving users, as any malicious activity is facilitated when the level of assurance of user identity is low. The trustworthiness of the transaction, denoted by trust(tr) is equally important. Indeed, fake reviews usually correspond to transactions that never occurred.
On the basis of the two measures above, we measure the trustworthiness trust(rs) of a feedback rs = ⟨u, d, v, k, t, I⟩ associated with a transaction tr, by the function trust(rs) = f(trust(u),trust(tr)) such that the higher trust(u) and trust(tr), the higher trust(rs). In this paper, we experiment as first attempt a simple function f which is the linear combination of the two contributions. Once trust(rs) has been computed, the score k of the feedback rs is corrected on the basis of the overall trustworthiness trust(rs) by means of a function g(trust(rs)). This function is build is such a way that the score k is much closer to the average score obtained by the service provider s as the value trust(rs) is low.
Specifically, g(k) = (α+trust(rs))·k , where α is s suitable (small) offset to P ′ (α+trust(rs′ ))
rs∈R(S) avoid null terms. This way, the mean computed over all the scores obtained by the
operator s is just the mean weighted by trust values (shifted by α) of each review. Therefore, we obtain the normalized feedback rs∗ as rs∗ = ⟨u, d, v, g(k), t, I⟩.
Let us explain now how trust(u), representing the level of assurance of iden- tity the user u authoring the review, is computed. Of course, a significant part of the current weakness of the reputation system of TripAdvisor is based on the weakness of its digital identity management system.
Concerning trust(u), we observe that the registration phase of TripAdvisor does not force the user to provide any non-self declared credential. Anyway, the possibility of registering via an existing Facebook profile is also allowed.
To compute the level of assurance of the identity, we consider in our model the following features (which are, in turn, numbers from 0 to 1).
		–  re (reviewer experience): it measures the seniority of the user in the system. 
		–  tc (text coherence): it measures the coherence of the known information about  the user (for example, the gender) and the text. 
		–  rc (review count): it measures the number of reviews of the user. This feature  is related to the fact that often fake reviews are done through accounts aimed to a specific goal (for example, for self-promotion or slandering attacks), so they are not reused massively, also to avoid the linkage with de-anonymizing information. 
		–  fi (Facebook identity): it measures the level of assurance of the Facebook identity of the user (it is trivially 0 if the TripAdvisor account is not asso- ciated with a Facebook profile). Concerning this feature, we argue that a fake reviewer does not have any interest in allowing linkage of her/his Tri- pAdvisor account with other (even fake) accounts, because probably tends to operate as much as possible in an anonymous way.  trust(u) is then obtained as a linear combination of the above components. To compute the level of assurance of the transaction trust(tr), we consider  in our model the following features (which are, again, numbers from 0 to 1). 
		–  dc (data coherence): it measures the coherence between the date of the review and the date of the transaction (this measure is based on the fact, according to our estimation, 50% of the reviews is done within 23 days, 75% within 34 days and 80% within 40). 
		–  ip (image proof): it measures the degree of proof given by posted images that the transaction really occurred (trivially, no posted images corresponds to 0, the presence of images recognized as coherent with the other posted by the majority of users corresponds to the maximum value). Indeed, the standard behavior of a faker is to hide as much as possible any information that could be a potential risk for de-anonymization, also the publication of images, which is a typical action done by honest reviewers to give a proof of their claims. 
– rl (review locality): it measures the presence of other reviews by the same user in the same location (city or region) if different from the place of residence corresponding to transactions experienced in the same period.
– or (operator reaction): it measures the presence of a reaction posted by the operator which is an outlier w.r.t. the standard behavior of the operator.
trust(tr) is also obtained as a linear combination of the above components. 3 Experiments
In this section, we describe the experiments carried out to validate the proposal: they are based on real-life reviews of restaurants extracted from the TripAdvisor site. First, we describe the data collection procedure and the error metrics used in the experiments. Then, we discuss how a score quantifying the (actual) quality of a restaurant has been computed (it is used as ground truth) and the method adopted to tune the weight of the reputation model parameters. Finally, we show the improvements obtained by our proposal in measuring the score of a restaurant.
3.1 Test Bed
For the study presented in this contribution, we consider data taken from Tri- pAdvisor. Data were collected in January 2017, by developing an ad-hoc scraping software and by using the available API to crawl information. The web scraping process was performed by a Python script that navigated through the restau- rants available on the Province of Lucca web page. The metadata related to a review, such as the language of the review, the rating, etc, were obtained by the available APIs. We also stored the reviewers’ profiles, which include, when available, the age and country of origin.
At the end of the extraction phase, we obtained a dataset composed of 1.499 restaurants, 60.613 reviewers, and 107.556 reviews. Each review judges the qual- ity of a restaurant by an integer score from 1 to 5.
In our experiments, we define the bias of a review w as Bw = |sw −GT r | , 5
where sw is the review score of the restaurant r, the operator |x| denotes the absolute value of the number x, and Qr (expected quality) is a real number in the interval [1, 5] representing the quality of the restaurant r, which is computed as the average of the scores of that restaurant. Bw represents how much the review score is far from the score of the restaurant and it is normalized w.r.t. the maximum score (i.e., 5). For example, Bw = 0 means that the review score is coherent with the quality of the restaurant.
We define review bias of a restaurant r as r Pni=0 |BWi |
RB= n (1)
https://www.tripadvisor.com/Tourism-g187898-Lucca Province of Lucca Tusc any-Vacations.html
where Wi is the i-th of the n reviews of the restaurant r. In words, it is used to quantify the amount of variation of the reviews of a restaurant w.r.t. the expected quality of that restaurant. For example, RBr = 0 means that all review scores coincide and their value reflects the quality of r.
Given a reputation model tu, we define its error % u Pmi=0 RBir
E= m ·100 (2)
where m is the number of restaurants used in the reputation model and ri is the
i-th one. Clearly, it is the average of the review bias computed for all restaurants.
Finally, to compare the accuracy of two reputation models t1 and t2, we
define the improvement % of t (w.r.t. t ) as I = E1−E2 ·100. Observe that the 1 2 1 E1
improvement can be negative in case the reputation model is less accurate than the compared one.
3.2 Parameter Setting
The reputation model presented in this paper uses only five of the parameters defined in Section 2, because they are the only ones evaluable from the collected dataset: two based on the identity of the reviewer and three based on the trans- action. They assume a rate from 0 to 1 and how their rate is computed is now discussed.
		f1  This parameter is related to the number of reviews done by the reviewer. We computed an average of 36 reviews for each reviewer and we assigned 1 to this parameter to reviewers with at least twice the average value. This values is linearly reduced to 0 for reviewers with only 1 review. 
		f2  This parameter is 1 if the reviewer signs in by Facebook, 0 otherwise. 
		f3  This parameter is 1 if the review contains at least an image, 0 otherwise. 
		f4  This parameter is related to length of the reviewer membership. As We have  reviewers with activity up to 180 months, we assigned 1 to the rate of ID2 = 1 to reviewers with at least 90 months of activity. This values is linearly reduced to 0 for reviewers registered very recently (clearly, recently w.r.t. the period in which data have been collected) 
		f5  This parameter is related to the coherence between review date and visit date.Wesetf5 =1iftimedelta<15days,elsef5 =.75iftimedelta<30 days, else f5 = .25 if time delta < 45 days, 0 otherwise.  In the next experiment, we analyze the performance of our model in which 
a single parameter is considered. The result of this experiment is reported in Fig. 1.(a) and shows that some parameters are useful to improve the reputation model (namely, f1, f3, and f4), others reduce its performance.
The next task is to combine all parameters and, for this purpose, we need to give a weight to each of them. Such weights are computed by applying a
Observe that this number includes many reviews that are not included in our dataset.
(a) Improvement of the reputation model (b) Performance of the proposed reputa- enabling only one parameter at a time tion model
Fig. 1: Experiment results
multivariable regression model with the five parameters f1 . . . f5 which returned the following weights w1 = 0.6068, w2 = 0.2520, w3 = 0.0605, w4 = 0.2864, w5 = 0.5778, where wx is the weight of the parameter fx with 1 ≤ x ≤ 5. These weights will be used in the next experiment.
3.3 Validation
In this experiment, we measure the performance of the reputation model of Tri- pAdvisor and the performance obtained by using the reputation model proposed in this paper, setting the parameter weight to the values obtained in Section 3.2. We measured the review bias for the first n reviews, with n ranging from 10 to 100: we limited the upper bound to 100 because, for higher values, the review bias is very low. In Fig. 1.(b), we report the improvement % of the proposed rep- utation model w.r.t. that of TripAdvisor. It is possible to see that our proposal always gives the best results: the improvement is higher when a restaurant has few reviews, that is when it is more vulnerable to fake reviews.
4 Conclusion
TripAdvisor, as well as many other reputation systems, suffers from self-promoting and slandering attacks typically performed by using fake accounts just created for this purpose, which post fake reviews not corresponding to real experiences. In this paper, we defined a new reputation model for the reviews of TripAdvisor. Our proposal aims at evaluate the dependability of a review on the basis of a level of assurance for both identity proofing and truth of the transaction. We validate our proposal by measuring the improvement obtained by enabling our reputation model on a real-life dataset reviews referring to restaurants of the Province of Lucca, Italy. Anyway, the paper is just aimed to give a general ex- perimental validation of the whole approach and, thus, of the global combination
of any feature. As a future work, a selective validation of the different features can be also performed.



Technological advances over the past decade have led to the proliferation of consumer
review websites such as Yelp.com, where consumers can share experiences about product
quality. These reviews provide consumers with information about experience goods, which have
quality that is observed only after consumption. With the click of a button, one can now acquire
information from countless other consumers about products ranging from restaurants to movies
to physicians. This paper provides empirical evidence on the impact of consumer reviews in the
restaurant industry.
It is a priori unclear whether consumer reviews will significantly affect markets for
experience goods. On the one hand, existing mechanisms aimed at solving information problems
are imperfect: chain affiliation reduces product differentiation, advertising can be costly, and
expert reviews tend to cover small segments of a market.
1 Consumer reviews may therefore
complement or substitute for existing information sources. On the other hand, reviews can be
noisy and difficult to interpret because they are based on subjective information reflecting the
views of a non-representative sample of consumers. Further, consumers must actively seek out
reviews, in contrast to mandatory disclosure and electronic commerce settings. 2
How do online consumer reviews affect markets for experience goods? Using a novel data
set consisting of reviews from the website Yelp.com and revenue data from the Washington
State Department of Revenue, I present three key findings: (1) a one-star increase in Yelp rating
leads to a 5-9 percent increase in revenue, (2) this effect is driven by independent restaurants;
ratings do not affect restaurants with chain affiliation, and (3) chain restaurants have declined in
1 For example, Zagat covers only about 5% of restaurants in Los Angeles, according to Jin and Leslie (2009).
2 For an example of consumer reviews in electronic commerce, see Cabral and Hortacsu (2010). For an example of
the impact of mandatory disclosure laws, see Mathios (2000), Jin and Leslie (2003), and Bollinger et al. (2010).
3
revenue share as Yelp penetration has increased. Consistent with standard learning models,
consumer response is larger when ratings contain more information. However, consumers also
react more strongly to information that is more visible, suggesting that the way information is
presented matters.
To construct the data set for this analysis, I worked with the Washington State Department of
Revenue to gather revenues for all restaurants in Seattle from 2003 through 2009. This allows
me to observe an entire market both before and after the introduction of Yelp. I focus on Yelp
because it has become the dominant source of consumer reviews in the restaurant industry. For
Seattle alone, the website had over 60,000 restaurant reviews covering 70% of all operational
restaurants as of 2009. By comparison, the Seattle Times has reviewed roughly 5% of
operational Seattle restaurants.
To investigate the impact of Yelp, I first show that changes in a restaurant’s rating are
correlated with changes in revenue, controlling for restaurant and quarter fixed effects.
However, there can be concerns about interpreting this as causal if changes in a restaurant’s
rating are correlated with other changes in a restaurant’s reputation that would have occurred
even in the absence of Yelp. This is a well-known challenge to identifying the causal impact of
any type of reputation on demand, as described in Eliashberg and Shugan (1997).
To support the claim that Yelp has a causal impact on revenue, I exploit the institutional
features of Yelp to isolate variation in a restaurant’s rating that is exogenous with respect to
unobserved determinants of revenue. In addition to specific reviews, Yelp presents the average
rating for each restaurant, rounded to the nearest half-star. I implement a regression
discontinuity (RD) design around the rounding thresholds, taking advantage of this feature.
Essentially, I look for discontinuous jumps in revenue that follow discontinuous changes in
4
rating. One common challenge to the RD methodology is gaming: in this setting, restaurants
may submit false reviews. I then implement the McCrary (2008) density test to rule out the
possibility that gaming is biasing the results. If gaming were driving the result, then one would
expect ratings to be clustered just above the discontinuities. However, this is not the case. More
generally, the results are robust to many types of firm manipulation.
Using the RD framework, I find that a restaurant’s average rating has a large impact on
revenue - a one-star increase leads to a 5-9 percent increase in revenue for independent
restaurants, depending on the specification. The identification strategy used in this paper shows
that Yelp affects demand, but is also informative about the way that consumers use information.
If information is costless to use, then consumers should not respond to rounding, since they also
see the underlying reviews. However, a growing literature has shown that consumers do not use
all available information (Dellavigna and Pollet 2007; 2010). Further, responsiveness to
information can depend not only on the informational content, but also on the simplicity of
calculating the information of interest (Chetty et al. 2009, Finkelstein 2009). Moreover, many
restaurants on Yelp receive upward of two hundred reviews, making it time-consuming to read
them all. Hence, the average rating may serve as a simplifying heuristic to help consumers learn
about restaurant quality in the face of complex information.
Next, I examine the impact of Yelp on revenues for chain restaurants. As of 2007, roughly
$125 billion per year is spent at chain restaurants, accounting for over 50% of all restaurant
spending in the United States. Chains share a brand name (e.g., Applebee’s or McDonald’s), and
often have common menu items, food sources, and advertising. In a market with more products
than a consumer can possibly sample, chain affiliation provides consumers with information
about the quality of a product. Because consumers have more information about chains than
5
about independent restaurants, one might expect Yelp to have a larger effect on independent
restaurants. My results demonstrate that despite the large impact of Yelp on revenue for
independent restaurants; the impact is statistically insignificant and close to zero for chains.
Empirically, changes in a restaurant’s rating affect revenue for independent restaurants but
not for chains. A standard information model would then predict that Yelp would cause more
people to choose independent restaurants over chains. I test this hypothesis by estimating the
impact of Yelp penetration on revenue for chains relative to independent restaurants. The data
confirm this hypothesis. I find that there is a shift in revenue share toward independent
restaurants and away from chains as Yelp penetrates a market.
Finally, I investigate whether the observed response to Yelp is consistent with Bayesian
learning. Under the Bayesian hypothesis, reactions to signals are stronger when the signal is
more precise (i.e., the rating contains a lot of information). I identify two such situations. First,
a restaurant’s average rating aggregates a varying number of reviews. If each review presents a
noisy signal of quality, then ratings that contain more reviews contain more information.
Further, the number of reviews is easily visible next to each restaurant. Consistent with a model
of Bayesian learning, I show that market responses to changes in a restaurant’s rating are largest
when a restaurant has many reviews. Second, a restaurant’s reviews could be written by high
quality or low quality reviewers. Yelp designates prolific reviewers with “elite” status, which is
visible to website readers. Reviews can be sorted by whether the reviewer is elite. Reviews
written by elite members have nearly double the impact as other reviews.
This final point adds to the literature on consumer sophistication in responses to quality
disclosure, which has shown mixed results. Scanlon et al. (2002), Pope (2009), and Luca and
Smith (2010) all document situations where consumers rely on very coarse information, while
6
ignoring finer details. On the other hand, Bundorf et al (2009) show evidence of consumer
sophistication. When given information about birth rates and patient age profiles at fertility
clinics, consumers respond more to high birth rates when the average patient age is high. This
suggests that consumers infer something about the patient mix. Similarly, Rockoff et al. (2010)
provide evidence that school principals respond to noisy information about teacher quality in a
way that is consistent with Bayesian learning. My results confirm that there is a non-trivial cost
of using information, but consumers act in a way that is consistent with Bayesian learning,
conditional on easily accessible information.
Overall, this paper presents evidence that consumers use Yelp to learn about independent
restaurants but not those with chain affiliation. Consumer response is consistent with a model of
Bayesian learning with information gathering costs. The introduction of Yelp then begins to
shift revenue away from chains and toward independent restaurants.
The regression discontinuity design around rounding rules offered in this paper will also
allow for identification of the causal impact of reviews in a wide variety of settings, helping to
solve a classic endogeneity problem. For example, Amazon.com has consumer reviews that are
aggregated and presented as a rounded average. RottenTomatoes.com presents movie critic
reviews as either “rotten” or “fresh,” even though the underlying reviews are assigned finer
grades. Gap.com now allows consumers to review clothing; again, these reviews are rounded to
the half-star. For each of these products and many more, there is a potential endogeneity
problem where product reviews are correlated with underlying quality. With only the underlying
reviews and an outcome variable of interest, my methodology shows how it is possible to
identify the causal impact of reviews.
7
2 Data
I combine two datasets for this paper: restaurant reviews from Yelp.com and revenue data
from the Washington State Department of Revenue.
2.1 Yelp.com
Yelp.com is a website where consumers can leave reviews for restaurants and other
businesses. Yelp was founded in 2004, and is based in San Francisco. The company officially
launched its website in all major west coast cities (and select other cities) in August of 2005,
which includes Seattle. It currently contains over 10 million business reviews, and receives
approximately 40 million unique visitors (identified by IP address) per month.
Yelp is part of a larger crowdsourcing movement that has developed over the past decade,
where the production of product reviews, software, and encyclopedias, among others are
outsourced to large groups of anonymous volunteers rather than paid employees. The appendix
shows trends in search volumes for Yelp, Trip Advisor, and Angie’s List, which underscores the
growth of the consumer review phenomenon.
On Yelp, people can read restaurant reviews and people can write restaurant reviews. In
order to write a review, a user must obtain a free account with Yelp, which requires registering a
valid email address. The users can then rate any restaurant (from 1-5 stars), and enter a text
review.
Once a review is written, anyone (with or without an account) can access the website for free
and read the review. Readers will come across reviews within the context of a restaurant search,
where the reader is trying to learn about the quality of different restaurants. Figure 1 provides a
8
snapshot of a restaurant search in Seattle. Key to this paper, readers can look for restaurants that
exceed a specified average rating (say 3.5 stars). Readers can also search within a food category
or location.
A reader can click on an individual restaurant, which will bring up more details about the
restaurant. As shown in Figure 2, the reader will then be able to read individual reviews, as well
as see qualitative information about the restaurants features (location, whether it takes credit
cards, etc).
Users may choose to submit reviews for many reasons. Yelp provides direct incentives for
reviewers, such as having occasional parties for people who have submitted a sufficiently large
number of reviews. Wang (2010) looks across different reviewing systems (including Yelp) to
analyze the social incentives for people who decide to submit a review.
2.2 Restaurant Data
I take the Department of Revenue data to be the full set of restaurants in the city of Seattle.
The data contains every restaurant that reported earning revenue at any point between January
2003 and October 2009. The Department of Revenue assigns each restaurant a unique business
identification code (UBI), which I use to identify restaurants. In total, there are 3,582 restaurants
during the period of interest. On average, there are 1,587 restaurants open during a quarter. This
difference between these two numbers is accounted for by the high exit and entry rates in the
restaurant industry. Approximately 5% of restaurants go out of business each quarter.
Out of the sample, 143 restaurants are chain affiliated. However, chain restaurants tend to
have a lower turnover rate. In any given quarter, roughly 5% of restaurants are chains. This can
be compared to Jin and Leslie (2009), who investigate chains in Los Angeles. Roughly 11% of
9
restaurants in their sample are chains. Both of these cities have substantially smaller chain
populations than the nation as a whole, largely because chains are more common in rural areas
and along highways.
The Department of Revenue divides restaurants into three separate subcategories, in
accordance with the North American Industry Classification System: Full Service Restaurants,
Limited Service Restaurants, and Cafeterias, Grills, and Buffets. Roughly two-thirds of the
restaurants are full service, with most of the others falling under the limited service restaurants
category (only a handful are in the third group).
2.3 Aggregating Data
I manually merged the revenue data with the Yelp reviews, inspecting the two datasets for
similar or matching names. When a match was unclear, I referred to the address from the
Department of Revenue listing. Table 1 summarizes Yelp penetration over time. By October of
2009, 69% of restaurants were on Yelp. To see the potential for Yelp to change the way firms
build reputation, consider the fact that only 5% of restaurants are on Zagat (Jin and Leslie, 2009).
The final dataset is at the restaurant quarter level. Table 2 summarizes the revenue and
review data for each restaurant quarter. The mean rating is 3.6 stars out of 5. On average, a
restaurant receives 3 reviews per quarter, with each of these reviewers having 245 friends on
average. Of these reviews, 1.4 come from elite reviewers. “Elite” reviewers are labeled as such
by Yelp based on the quantity of reviews as well as other criteria.
One challenge with the revenue data is that it is quarterly. For the OLS regressions, I simply
use the average rating for the duration of the quarter. For the regression discontinuity, the
process is slightly more complicated. For these observations, I do the following. If the rating
10
does not change during a given quarter, then I leave it as is. If the rating does cross a threshold
during a quarter, then I assign the treatment variable based on how many days the restaurant
spent on each side of the discontinuity. If more than half of the days were above the
discontinuity, then I identify the restaurant as above the discontinuity.3
3 Empirical Strategy
I use two identification strategies. I implement a regression discontinuity approach to
support the hypothesis that Yelp has a causal impact. I then apply fixed effects regressions to
estimate the heterogeneous effects of Yelp ratings.
3.1 Impact of Yelp on Revenue
The first part of the analysis establishes a relationship between a restaurant’s Yelp rating and
revenue. I use a fixed effects regression to identify this effect. The regression framework is as
follows:
ln⁡ (𝑅𝑒𝑣𝑒𝑛𝑢𝑒𝑗𝑡) = 𝛽⁡𝑟𝑎𝑡𝑖𝑛𝑔𝑗𝑡 + 𝛼1𝑗 + 𝛼2𝑡 + 𝜖𝑗𝑡
where rating is ln⁡ (𝑅𝑒𝑣𝑒𝑛𝑢𝑒𝑗𝑡) is the log of revenue for restaurant j in quarter t, ⁡𝑟𝑎𝑡𝑖𝑛𝑔𝑗𝑡 is the
rating for restaurant j in quarter t. The regression also allows for year and restaurant specific
unobservables. 𝛽 is the coefficient of interest, which tells us the impact of a 1 star improvement
in rating on a restaurant’s revenue. While a positive coefficient on rating suggests that Yelp has
a causal impact, there could be concern that Yelp ratings are correlated with other factors that
3 An alternative way to run the regression discontinuity would be to assign treatment based on the restaurant’s
rating at the beginning of the quarter.
11
affect revenue. To support the causal interpretation, I turn to a regression discontinuity
framework.
3.2 Regression Discontinuity
Recall that Yelp displays the average rating for each restaurant. Users are able to limit
searches to restaurants with a given average rating. These average ratings are rounded to the
nearest half a star. Therefore, a restaurant with a 3.24 rating will be rounded to 3 stars, while a
restaurant with a 3.25 rating will be rounded to 3.5 stars, as in Figure 5. This provides variation
in the rating that is displayed to consumers that is exogenous to restaurant quality.
I can look at restaurants with very similar underlying ratings, but which have a half-star gap
in what is shown to consumers. To estimate this, I restrict the sample to all observations in
which a restaurant is less than 0.1 stars from a discontinuity. This estimate measures the average
treatment effect for restaurants that benefit from receiving an extra half star due to rounding. I
also present estimates for alternative choices of bandwidth.
3.2.1 Potential Outcomes Framework
The estimation is as follows. First, define the binary variable T:
𝑇 = {0⁡𝑖𝑓⁡𝑟𝑎𝑡𝑖𝑛𝑔⁡𝑓𝑎𝑙𝑙𝑠⁡𝑗𝑢𝑠𝑡⁡𝒃𝒆𝒍𝒐𝒘⁡𝑎⁡𝑟𝑜𝑢𝑛𝑑𝑖𝑛𝑔⁡𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑⁡(𝑠𝑜⁡𝑖𝑠⁡𝑟𝑜𝑢𝑛𝑑𝑒𝑑⁡𝑑𝑜𝑤𝑛)
1⁡𝑖𝑓⁡𝑟𝑎𝑡𝑖𝑛𝑔⁡𝑓𝑎𝑙𝑙𝑠⁡𝑗𝑢𝑠𝑡⁡𝒂𝒃𝒐𝒗𝒆⁡𝑎⁡𝑟𝑜𝑢𝑛𝑑𝑖𝑛𝑔⁡𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑⁡⁡(𝑠𝑜⁡𝑖𝑠⁡𝑟𝑜𝑢𝑛𝑑𝑒𝑑⁡𝑢𝑝)⁡⁡⁡⁡⁡
For example, T = 0 if the rating is 3.24, since a Yelp reader would see 3 stars as the average
rating. Similarly, T=1 if the rating is 3.25, since a Yelp reader would see 3.5 stars as the average
rating.
The outcome variable of interest is ln (Revenuejt). The regression equation is then simply:
ln(𝑅𝑒𝑣𝑒𝑛𝑢𝑒 𝑗𝑡) = 𝛽⁡𝑇 𝑗𝑡 + 𝛾⁡𝑞𝑜𝑗𝑡 + 𝛼1𝑗 + 𝛼2𝑡 + 𝜖𝑗𝑡⁡
12
where β is the coefficient of interest. It tells us the impact of an exogenous one-half star increase
in a restaurant’s rating on revenue. The variable 𝑞𝑜𝑗𝑡 is the unrounded average rating. The
coefficient of interest then tells us the impact of moving from just below a discontinuity to just
above a discontinuity, controlling for the continuous change in rating.
In the main specification, I include only the restricted sample of restaurants that are less
than 0.1 stars away from a discontinuity. To show that the result is not being driven by choice of
bandwidth, I allow for alternative bandwidths. To show that the result is not being driven by
non-linear responses to continuous changes in rating, I allow for a break in response to the
continuous measure around the discontinuity. I also allow for non-linear responses to rating. I
then perform tests of identifying assumptions.
3.3 Heterogeneous impact of Yelp
After providing evidence that Yelp has a causal impact on restaurant revenue, I investigate
two questions regarding heterogeneous impacts of Yelp. First, I test the hypothesis that Yelp has
a smaller impact on chains. The estimating equation is as follows:
ln⁡ (𝑅𝑒𝑣𝑒𝑛𝑢𝑒𝑗𝑡) = 𝛽⁡𝑟𝑎𝑡𝑖𝑛𝑔𝑗𝑡 + 𝛿⁡𝑟𝑎𝑡𝑖𝑛𝑔⁡𝑋⁡𝑐ℎ𝑎𝑖𝑛⁡𝑗𝑡 + 𝛼1𝑗 + 𝛼2𝑡 + 𝜖𝑗𝑡
The coefficient of interest is then 𝛿. A negative coefficient implies that ratings have a smaller
impact on revenue for chain restaurants.
I then test whether consumer response is consistent with a model of Bayesian learning.
The estimating equation is as follows:
ln⁡ (𝑅𝑒𝑣𝑒𝑛𝑢𝑒𝑗𝑡) = 𝛽⁡𝑟𝑎𝑡𝑖𝑛𝑔𝑗𝑡 + ⁡𝛾⁡𝑟𝑎𝑡𝑖𝑛𝑔⁡𝑋⁡𝑛𝑜𝑖𝑠𝑒⁡𝑗𝑡 + 𝛼1𝑗 + 𝛼2𝑡 + 𝜖𝑗𝑡⁡
13
The variable 𝑟𝑎𝑡𝑖𝑛𝑔⁡𝑋⁡𝑛𝑜𝑖𝑠𝑒⁡𝑗𝑡 interacts a rating with the amount of noise in the rating. A
Bayesian model predicts that if the signal is less noisy, then the reaction should be stronger. The
variable 𝑟𝑎𝑡𝑖𝑛𝑔⁡𝑋⁡𝑝𝑟𝑖𝑜𝑟⁡𝑏𝑒𝑙𝑖𝑒𝑓𝑠⁡𝑗𝑡 interacts a rating with the precision of prior beliefs about
restaurant quality. Bayesian learning would imply that the market reacts less strongly to new
signals when prior beliefs are more precise. All specifications will include restaurant and year
fixed effects.
Empirically, I will identify situations were ratings contain more and less information and
where prior information is more and less precise. I will then construct the interaction terms
between these variables and a restaurants rating.
There are two ways in which I measure noise. First, I consider the number of reviews
that have been left for a restaurant. If each review provides a noisy signal of quality, then the
average rating presents a more precise signal as there are more reviews left for each restaurant.
Bayesian learners would then react more strongly to a change in rating when there are more total
reviews. Second, I consider reviews left by elite reviewers, who have been certified by Yelp. If
reviews by elite reviewers contain more information, then Bayesian learners should react more
strongly to them.
4 Impact of Yelp on Revenue
Table 3 establishes a relationship between a restaurant’s rating and revenue. A one-star
increase is associated with a 5.4% increase in revenue, controlling for restaurant and quarter
specific unobservables. The concern with this specification is that changes in a restaurant’s
rating may be correlated with other changes in a restaurant’s reputation. In this case, the
coefficient on Yelp rating might be biased by factors unrelated to Yelp.
14
To reinforce the causal interpretation, I turn to the regression discontinuity approach. In this
specification, I look at restaurants that switch from being just below a discontinuity to just above
a discontinuity. I allow for a restaurant fixed effect because of a large restaurant-specific
component to revenue that is fixed across time. Figure 4 provides a graphical analysis of
demeaned revenues for restaurants just above and just below a rounding threshold. One can see
a discontinuous jump in revenue. Table 4 reports the main result, with varying controls. Table 4
considers only restaurants that are within a 0.1-star radius of a discontinuity. Table 5 varies the
bandwidth.
I find that an exogenous one-star improvement leads to a roughly 9% increase in revenue.
(Note that the shock is one-half star, but I renormalize for ease of interpretation). The result
provides support to the claim that Yelp has a causal effect on demand. In particular, whether a
particular restaurant is rounded up or rounded down should be uncorrelated with other changes in
reputation outside of Yelp.
The magnitude of this effect can be compared to the existing literature on the impact of
information. Gin and Leslie (2003) show that when restaurants are forced to post hygiene report
cards, a grade of A leads to a 5% increase in revenue relative to other grades. In the online
auction setting, Cabral and Hortacsu (2010) show that a seller experiences a 13% drop in sales
after the first bad review. In contrast to the electronic commerce setting, Yelp is active in a
market where (1) other types of reputation exist since the market is not anonymous (and many
restaurants are chain-affiliated), (2) there may be a high cost to starting a new firm or changing
names, leaving a higher degree of variation in rating, and (3) consumers must actively seek
information, rather than being presented with it at the point of purchase.
15
In addition to identifying the causal impact of Yelp, the regression discontinuity estimate is
information about the way that consumers use Yelp. First, it tells us that Yelp as a new source of
information is becoming an important determinant of restaurant demand. The popularization of
the internet has provided a forum where consumers can share experiences, which is becoming an
important source of reputation. Second, the mean rating is a salient feature in the way that
consumers use Yelp. Consumers respond to discontinuous jumps in the average rating.
Intuitively, the average rating provides a simple feature that is easy to use. Third, this implies
that consumers do not use all available information, but instead use the rounded rating as a
simplifying heuristic. Specifically, if attention was unlimited, then consumers would be able to
observe changes to the mean rating based on the underlying reviews. Then the rounded average
would be pay-off irrelevant. Instead, consumers use the discontinuous rating, which is less
informative than the underlying rating but also less costly (in terms of time and effort) to use.
4.1 Identifying Assumptions
This regression discontinuity approach heavily relies on random assignment of restaurants to
either side of the rounding thresholds. Specifically, the key identifying assumption is that as we
get closer and closer to a rounding threshold, all revenue-affecting predetermined characteristics
of restaurants become increasingly similar. Restricting the sample to restaurants with very
similar ratings, we can simply compare the revenues of restaurants that are rounded up to the
revenues of restaurants that are rounded down.
This helps to avoid many of the potential endogeneity issues that occur when looking at the
sample as a whole. In particular, restaurants with high and low Yelp scores may be very
16
different. Even within a restaurant, reputational changes outside of Yelp may be correlated with
changes in Yelp rating over time. However, the differences should shrink as the average rating
becomes more similar.
For restaurants with very similar ratings, it seems reasonable to assume that restaurants
changes that are unrelated to Yelp would be uncorrelated with whether a restaurant’s Yelp rating
is rounded up or rounded down. The following section addresses potential challenges to
identification.
4.1.1 Potential Manipulation of Ratings
One challenge for identification in a regression discontinuity design is that any threshold that
is seen by the econometrician might also be known to the decision makers of interest. This can
cause concerns about gaming, as discussed in McCrary (2008). In the Yelp setting, the concern
would be that certain types of restaurants submit their own reviews in order to increase their
revenue. This type of behavior could bias the OLS estimates in this paper if there is a correlation
between a firm’s revenue and decision to game the system. The bias could go in either direction,
depending on whether high revenue or low revenue firms are more likely to game the system. In
this section, I address the situation that could lead to spurious results. I then argue that selective
gaming is not causing a spurious correlation between ratings and revenues in the regression
discontinuity framework.
In order for the regression discontinuity estimates to be biased, it would have to be the case
that restaurants with especially high (or alternatively with especially low) revenue are more
likely to game the system. This is certainly plausible. However, it would also have to be the
case that these restaurants stop submitting fake reviews once they get above a certain
17
discontinuity. In other words, if some restaurants decided to submit fake reviews while others
did not, the identification would still be valid.
In order to invalidate the regression discontinuity identification, a restaurant would have to
submit inflated reviews to go from a rating of 2.2 stars, only to stop when it gets to 2.4 stars.
However, if a restaurant stopped gaming as soon as it jumped above a discontinuity, the next
review could just drop it back down. While the extent of gaming is hard to say, it is a very
restrictive type of gaming that would lead to spurious estimates.
I offer two further arguments against the gaming hypothesis: one economic and the other
statistical. First, suppose the concern is that restaurants are gaming in this sophisticated manner,
leading to a spurious impact of rating where none exists. This argument becomes circular
because if no effect exists, then restaurants should not have the incentive to invest in gaming.
Therefore even the existence of gaming would require that Yelp has a causal effect on revenue.
The second piece of evidence against the gaming hypothesis is based on a test offered by
McCrary (2008). The intuition of the test is as follows. Suppose that restaurants were gaming
Yelp in a way that would bias the results. Then, one would expect to see a disproportionately
large number of restaurants just above the rounding thresholds.
I construct the test in the following way. I begin with a dataset at the restaurant / review
level. For example, a restaurant that has five reviews would have five observations. The
variable of interest would be the average rating after each review. If there was gaming, there
should be “too many” observations with ratings just above rounding thresholds.
To formally test for this, I sum the number of observations for each 0.05-star interval, and
compute the probability mass for each interval. I create a binary variable to indicate bins that fall
just above a rounding threshold (e.g., 3.25-3.3 stars, 3.75-3.8 stars). The dependent variable is
18
the probability mass, and the independent variable is the indicator for bins that fall just above the
discontinuity.
Table 6 presents the results of this test. The test shows that there is not any clustering of
restaurants just above the discontinuity, suggesting that manipulation is not an issue with the
regression discontinuity design.
5 The Impact of Yelp on Chains
How does the introduction of a new technology that increases information flow affect
restaurants with chain affiliation? Historically, chain affiliation is valuable precisely because it
reduces uncertainty about restaurant quality. Consumer reviews are coming to serve a similar
purpose.
There are two ways in which Yelp ratings might affect chains. First, a chain’s rating on Yelp
may have an effect on revenue. Second, Yelp may cause an overall shift in demand between
chains and independent restaurants if Yelp is providing more information about independent
restaurants than about chains. In this section, I investigate both effects.
5.1 Do Ratings Affect Chains?
Table 7 presents the differential impact of Yelp ratings on chain restaurants. While ratings
have a large impact on revenue overall, the effect is being driven entirely from independent
restaurants. Because chains already have relatively little uncertainty about quality, their demand
does not respond to consumer reviews
5.2 Do Consumer Reviews Crowd Out Demand for Chains?
19
Given the differential impact of Yelp on chains and independent restaurants, one might
expect chains to become less popular after the introduction of Yelp. This is because the
increased information about independent restaurants leads to a higher expected utility conditional
on going to an independent, restaurant. Hence Yelp should not only shift demand between
independent restaurants, it should also increase the value of going to an independent restaurant
relative to a chain.
Consistent with this, table 8 shows that chains experienced a decline in revenue relative to
independent restaurants in the post-Yelp period. Higher Yelp penetration leads to an increase in
revenue for independent restaurants, but a decrease in revenue for chain restaurants.
One may be concerned with this specification if chain restaurants had been trending
downward in the period before Yelp was introduced. To address this concern, I show that the
result is robust to the inclusion of chain-specific time trends.
6 Evidence of Bayesian Learning
How do consumers update beliefs based on information obtained from consumer reviews?
On the one hand, a standard model of Bayesian learning predicts that the market would react
more strongly when ratings contain more precise information and when prior beliefs are less
precise. On the other hand, we have already seen that consumers use the average rating as a
simplifying heuristic. This may cast doubt on the sophistication of consumer response.
It is possible to test for Bayesian learning, taking a restaurant’s rating as a public signal of
quality. The market response to the signal depends on two things: consumers’ prior beliefs about
product quality, and the precision of the signal. The precision of information contained in user
reviews depends on the number of reviews and the credibility of the reviewers.
20
In this section, I identify situations where the signal is more and less precise in order to test
for Bayesian learning.
6.1 Number of Reviews
If each consumer review presents a noisy signal of quality, then having many reviews should
cause the overall rating to contain more information and hence have a larger impact. Table 9
shows that this is in fact the case.
The first column looks at all restaurants, and shows that a change in a restaurant’s rating has
50% more impact when the restaurant has at least 50 reviews (compared to a restaurant with
fewer than 10 reviews). However, this interpretation could be concerning if restaurants that are
more rating-sensitive receive more reviews. To allay this concern, I restrict the sample to
restaurants that have at least 50 reviews as of October 2009 (column 2). I then consider how
responsive these restaurants are to changes in rating as they receive more reviews.
Under this specification, a restaurant with at least 50 reviews is roughly 20% more rating
sensitive than when it had fewer than 10 reviews.
6.2 Certified Reviewers
Consumer reviews are written by a non-representative sample of voluntary reviewers who
often have little or no connection to the reader. In order to find a review useful, a consumer must
find it relevant, accurate, and credible. One way to achieve this is to certify the quality of a
reviewer.
Yelp has a reviewer credentialing program, where they formally certify certain reviewers
who have written a lot of reviews that Yelp has deemed helpful. These reviewers are marked as
21
“elite,” and in addition to knowing whether a reviewer was elite, readers can filter to only look at
reviews by elite reviewers.
If elite certification gives reviewers a reputation for leaving informative reviews, then
reviews by elite members should have a larger impact. Consistent with the Bayesian hypothesis,
Table 10 shows that elite reviewers have roughly double the impact of other reviewers. Despite
the fact that the econometrician cannot observe the criteria for certifying a reviewer as elite, this
suggests a strong role for reviewer reputation.
An alternative explanation of this result is that Yelp simply certifies reviewers who are better
at predicting average consumer preferences. There are several difficulties with this
interpretation. First, Yelp does not have access to revenue data at the restaurant level, so this
would require Yelp to know consumer preferences. Second, if Yelp knew the distribution of
preferences over restaurants, they could simply announce them. Third, the regression includes
restaurant fixed effect. In order for the result to be spurious, elite reviewers would then have to
be more likely to review restaurants whose reputation is about to improve. To some extent, this
seems plausible. However, if it is, then rational consumers should be responding more heavily to
elite reviews, which are then more indicative of a restaurant’s reputation. This argument would
therefore not nullify the result. Further, I find that elite reviewers only have an effect after
becoming certified as elite.
A second way to think of certifying reviewer quality is through the number of “friends” the
reviewer has. Yelp reviewers are able to form online connections, called “friends” with other
reviewers. Having many friends might plausibly signal that a reviewer writes precise reviews, or
has tastes that reflect popular opinions. Empirically, I estimate this by weighting the overall
22
rating by the number of friends each reviewer has. I find that the number of friends each
reviewer has does not affect the impact of a review.
7 Discussion
The overall message of this paper is simple. Online consumer review websites improve the
information available about product quality. The impact of this information is larger for
products of relatively unknown quality. As this information flow improves, other forms of
reputation such as chain affiliation should continue to become less influential. On the consumer
side, simplifying heuristics and signals of reviewer quality seem to increase the impact of quality
information. In this section, I put some of the results into broader context and discuss possible
areas of future work.
7.1 Comparing Consumer Reviews with Mandatory Disclosure
This paper shows that a one-star increase leads roughly to a 9% increase in revenue. One
relevant comparison is between consumer reviews and mandatory disclosure laws. Jin and
Leslie (2003) find that a restaurant whose hygiene report card grade moves from a B to an A
experiences a 5% increase in revenue relative to other grades. Bollinger, Leslie, and Sorensen
(2010) find that calorie posting laws cause consumers to consumer 6% fewer calories at
Starbucks.
Ultimately, the policy goal of quality disclosure laws is to (1) provide information to
consumers, so that they can make better decisions and (2) hold firms accountable. This paper
suggests that consumer review websites can be equally as effective at altering demand, although
there is no hard evidence on the correlation between Yelp ratings and more objective quality
23
measures. In ongoing work, I am estimating the correlation between Yelp ratings and other
objective quality measures.
7.2 Comparing Yelp and Other Reviews
Clearly Yelp is not the only way in which consumers learn about restaurant quality.
However, Yelp is striking in the sheer number of restaurants that contain non-trivial numbers of
reviews. Appendix 4 shows the percent of restaurants covered by different review systems in
urban areas. As discussed, Yelp currently contains reviews of 70% of restaurants in Seattle. In
contrast, Zagat is only a 5% sample (Gin and Leslie 2009) in Los Angeles. My own data shows
that the Seattle Times - a local paper that also reviews restaurants - contains even fewer
restaurants, as does the magazine Food & Wine.
7.3 Comparative Incentive Problems of Consumer Reviews and Chains
Chain affiliation helps to increase the amount of information available about restaurant
quality. However, chain affiliation can also lead to free-riding (Jin and Leslie 2009) and high
monitoring costs (Kaufmann and Lafontaine 1994). Consumer reviews may reduce these
incentive problems. This is one reason why consumer demand is shifting from chain to
independent restaurants in the period following the introduction of Yelp. On the other hand,
consumer reviews create separate incentive issues, such as an underprovision problem (Avery
1999) and selection of reviewer.
24
7.4 Welfare Gains from Yelp
It seems uncontroversial to assert that providing this information to consumers might
improve welfare in various ways. As evidence, I discuss two results.
First, Yelp causes demand to shift from chains to independent restaurants. By revealed
preference, consumers’ expected utility from going to independent restaurants must then be
higher. This can be viewed as a welfare gain resulting from either better restaurants or better
sorting between consumers and restaurants.
Second, revenue is a key determinant of a restaurant’s decision to exit. Hence, Yelp may
have a long-run effect on exit behavior of firms. Assuming Yelp measures are a reasonable
measure of true quality, then Yelp may help to drive worse restaurants out of business, which
would be a second source of welfare gain. In ongoing work, I am estimating the relationship
between Yelp and exit decisions.




1. INTRODUCTION
The number of online reviews is currently increasing. For example, TripAdvisor provides over 40 million traveler reviews and ranked lists of over 125,000 visitor attractions, 450,000 hotels, and 600,000 restaurants (Jeacle & Carter, 2011). Thus, finding an answer to the question “What is the motiva- tion behind posting online reviews?” is valu- able not only from an industry viewpoint but also from an academic one.
Yoo and Gretzel (2011) found that women are more motivated by their desire to help a company, whereas men are motivated by their desire to prevent others from falling into traps. Similarly, Öğüta and Cezara (2012) found that higher ratings and lower prices increase the propensity to post reviews. Complaints about bad experiences also motivate customers to post reviews, and are likely to result in bad reviews. Sparks and Browning (2010) stated that customers who experience service failure tend to spread negative word-of-mouth (WOM)
 Markus Schuckert, Assistant Professor, School of Hotel and Tourism Management, The Hong Kong Polytechnic University, 17 Science Museum Road, TST-East, Kowloon, Hong Kong SAR (E-mail: markus.schuckert@polyu.edu.hk).
Xianwei Liu, PhD Candidate, Harbin Institute of Technology, School of Management, 92 Xidazhi Street, Harbin, 150001, China (E-mail: liuxianwei@126.com).
Rob Law, Professor, School of Hotel and Tourism Management, The Hong Kong Polytechnic University, 17 Science Museum Road, TST-East, Kowloon, Hong Kong SAR (E-mail: rob.law@polyu.edu.hk).
Address correspondence to: Markus Schuckert, School of Hotel & Tourism Management, The Hong Kong Polytechnic University, 17 Science Museum Road, TST-East, Kowloon, Hong Kong SAR
(E-mail: markus.schuckert@polyu.edu.hk).
The authors acknowledge the financial support of the Hong Kong Polytechnic University [grant number A-PM08].
1
2 JOURNAL OF TRAVEL & TOURISM MARKETING
and have the potential to influence the reputa- tion of a brand or firm. The motivation for posting negative WOM can be to take revenge or warn others (Wetzer, Zeelenberg, & Pieters, 2007). Restaurant service employees who create a good experience can also trigger positive WOM, which is motivated by the desire to support the employees and the restaurant, or simply to express personal positive feelings (Jeong & Jang, 2011).
The research stream is heading in the direc- tion of external factors to discover the triggers and motivations behind such activities. Empirical findings showed that online travel reviewers providing feedback are motivated by their desire to support the service provider and to push for improvements in service qual- ity. They also want to complain about bad experiences and express their feelings as well as concern for future consumers and their experiences. However, existing literature does not include studies that focus on how internal factors can motivate travelers to post online reviews; an internal factor can, for example, take the form of a reward system on the platforms where reviewers can post reviews or ratings.
Online reviewers can also be motivated to post more reviews by earning higher-level badges that are offered by the reward systems of websites. Reviewers who create more posts can thus obtain a more prestigious online badge. Therefore, frequent posting offers the opportunity to ascend the reputation ladder and demonstrate that one’s contributions are based on experience and expertise. A high-level badge signals that the reviewer is authoritative, senior, and experienced (Mkono, 2012; Yoo & Gretzel, 2010). Users of e-commerce platforms mainly comprise reviewers and readers. Reviewers are contributors who write numer- ous reviews that provide potential customers (readers of these reviews) with some or all the information they need to make “the right” decisions (Mauri & Minazzi, 2013). Nearly all e-commerce platforms offer these functions, systems, and incentive hierarchies to encou- rage their members to post more reviews and, in turn, benefit their business (Sparks & Browning, 2011).
These complex systems of online rewards, such as badges, aim to encourage users to con- tribute more content to increase their status or reach different levels. However, the initial meaning of giving a review changes in terms of contributing to a system to increase personal value (Zhang, Ye, Law, & Li, 2010). Thus, an important task is to ask whether and how these reward systems change the behavior of contri- butors and readers. We also need to understand the effect of this system on customers and its wider implications for the industry. On this basis, we examine whether different online badge levels affect the online behavior of both reviewers and readers. This question itself implies that online badge levels may have an effect on these groups, which leads to a ques- tion about the difference and diversity of actions. That is, do reviewers with high badges act differently from those with less prestigious badges when providing ratings? The findings of this study can also offer insights into the answer to the question with regard to the kind of online reviews that readers prefer.
Few studies have focused on value creation or the relationship between website badges and review ratings, with most of the current litera- ture focusing on the helpfulness of online reviews. The findings of such work are gener- ally focused on which reviews are considered more helpful and likely to attract more votes (Black & Kelley, 2009; Lee, Law, & Murphy, 2011; Zhang & Tran, 2010), and what kind of review is considered more authentic (Weimann, Tustin, Vuuren, & Joubert, 2007). In our view, understanding the effect of these reward sys- tems on behavior and identifying the prefer- ences of users are important. Reviewer badges are more than just a status symbol. In the online environment, reviewers may change their beha- vior on the website as their badge level rises. In this research, we focus on the effect of badges, which represent travel experience and serve as status symbols, on the ratings and value creation of reviewers. Specifically, we examine whether reviewers with high-level badges post moderate ratings, avoid extreme ratings, or create more helpful reviews.
In the hospitality industry, online ratings are important, and the number of websites that

provide such ratings has recently increased dramatically. This situation is due to the spe- cial characteristic (namely the intangibility) of services provided by the industry, such as offering hotel rooms. These services cannot be tested or used in advance and then returned if the customer is not satisfied. In this study, we use the reward system of TripAdvisor as a source of data set to test our hypotheses and answer the question: Do the online incentive hierarchies work? Thus, this research differs from other current studies. For example, as stated before, empirical findings have shown that online reviewers providing feedback are motivated by a desire to support the service provider and to push for improvements in ser- vice quality, as well as a concern for future consumers and their experiences (Jeong & Jang, 2011; Yoo & Gretzel, 2008, 2011). Sparks and Browning (2010) found that custo- mers who experience service failure are moti- vated to spread negative WOM and have the potential to influence the reputation of a brand or firm.
In the following, we provide an overview of recent literature and other relevant findings and develop our hypothesis based on these insights. We then outline our research design and explain the sampling and data collection procedures. Next, we present the results and discuss the findings, including an analysis of the implications for users, industry, and researchers. The study concludes with a criti- cal view of its limitations and opportunities for future research.
Relating to online environment and based on the work of Berger, Cohen, and Zelditch (1972) and Thye (2000), Lampel and Bhalla (2007, p. 437) perceive status seeking as activities that are “designed to improve an actor’s standing in a group, and is therefore judged by the degree to which associated activities result in increasing prestige, honor, or deference”. Status seeking can be externally motivated, specifi- cally, seeking economic and social advantage, or internally motivated for psychological and emotional reasons (Perretti & Negro, 2006). Status seeking is linked to self-image (Köszegi, 2006), gaining public recognition (Moldovanu, Sela, & Shi, 2007; Rustichini, 2008), or outperforming others (Dohmen, Falk, Fliessbach, Chowdhury & Sheremeta, 2012; Dohmen, Falk, Fliessbach, Sunde, & Weber, 2011).
Users who post reviews are more likely to be status seeking and to engage in conspicuous consumption than users who just consume the content (Cowan, Cowan, & Swann, 2004). Chen et al. (2011) found that demonstrating expertise, experience, and social status is an important part of virtual communities. Hence, status seeking, reputation gaining, and conspic- uous consuming are phenomena that are observed offline and online.
2.2. Reviewer Badges and Rating Behavior
Many types of website employ a badge sys- tem, such as e-commerce platforms (eBay, Amazon, and Taobao), online travel agents (Ctrip, TripAdvisor, and Agoda), and online question-and-answer sites (such as Yahoo! Answers, Answerbag, and StackExchange). Antin and Churchill (2011) found that badges provide information from which a reputation assessment can be made. Badges display a user’s choice, expertise, and past experience (Kollock, 1999). Furthermore, badges show engagement, experience, and expertise, as well as signaling trustworthiness within the virtual community. Relating to status, badges can be motivating as status symbols (Antin & Churchill, 2011) or, in Lampel and Bhalla’s
2.
BACKGROUND AND HYPOTHESES
2.1. Behavior
Status-Seeking Theory and Online
The underlying motivation for posting reviews is a psychological incentive for indivi- duals to disseminate positive WOM or satisfac- tion, that is, to gain social approval or self- approval (Chen, Fay, & Wang, 2011; Fehr & Falk, 2002), to signal their expertise and experi- ence, or to gain social status (Hennig-Thurau, Gwinner, Walsh, & Gremler, 2004).
Schuckert, Liu, and Law 3

4 JOURNAL OF TRAVEL & TOURISM MARKETING
(2007, p. 437) understanding, as an “ego reward” or an emotional good that a user “can accumulate as a result of acquired status”. Badges provide personal affirmation by serving as reminders of past achievements much like trophies on a mantelpiece, marking milestones, and providing evidence of past success (Antin & Churchill, 2011). Badges are a symbol of reputation, as the online community may view them as an indicator of expertise and experi- ence, which may then influence their future behavior (Antin & Churchill, 2011; Meng, Webster, & Butler, 2013).
The badge level represents a participant’s sta- tus in the online travel community. An individual may therefore seek to earn higher badge levels in order to gain public recognition, which results in certain contribution behaviors (Thompson, 2005). The badge systems of different websites work in similar ways. For example, participants in question-and-answer sites can earn badges when their answers are accepted or when they accomplish specific tasks. Reviewers who con- tribute to e-commerce platforms and online tra- vel agencies (OTAs) can earn badges, tags, or medals by posting more reviews (Butler & Wang, 2012). The purpose of badge systems is to engage members and encourage greater parti- cipation (Li, Huang, & Cavusoglu, 2012). In addition, based on theories of status seeking and self-presentation, providing opinion, advice, and experiences in online platforms is heavily influenced by the desire for status as an intrinsic motivation and external non-monetary reward (Lampel & Bhalla, 2007). Non-monetary-based mechanisms, such as titles, stars, votes, and badges, seem to be more effective than mone- tary-based mechanisms (Li et al., 2012; Shah, Oh, & Oh, 2008).
TripAdvisor categorizes its badges into five levels based on reviewer activity, as shown in Figure 1. Users who have posted fewer than
three times are not recognized. Three to five reviews are required to gain the status of “reviewer”. The thresholds for subsequent cate- gories are exponential. After six reviews, one earns the status of “senior reviewer”; with 11 reviews, one becomes a “contributor”; with more than 20 reviews, one becomes a “senior contributor”; and with more than 49 contribu- tions, one becomes a “gold member” or “top contributor”. A rising status is also denoted by assigning a symbolically more “valuable” gra- phic to each stage, with a light green star at the bottom of the scale and a solid gold star at the top.
A reviewer posts a comment and rating by using his or her account and username. Every subsequent review posted using the same account will be credited to that user’s reviewing statistics. If the number of reviews crosses one of the predefined thresholds, then a new badge status is assigned to that account. Reviewers can earn a higher badge by posting more reviews. Thus, the badge level is an indication of a reviewer’s travel experience (Chen & Huang, 2013). The number of reviews is also an indi- cator of travel activity and frequency, because visiting more hotels means that one can contri- bute more reviews. The rating platforms usually allow only customers with an actual booking (and supposedly a real stay) to file a review (Vermeulen & Seegers, 2009). An increasing number of booking engines and online booking platforms are now linked to TripAdvisor. Customer data, booking platforms, and mail robots are integrated systems, which means that customers can be asked automatically to rate their stay after a trip has been completed (Bronner & De Hoog, 2011). Numerical ratings for online reviews typically range from one star to five stars (Zhang, Zhang, Wang, Law, & Li, 2013). A very low rating (one star) indicates an extremely negative view of the product or
FIGURE 1. Example for Badge Levels
 Source. TripAdvisor

service, a very high rating (five stars) reflects an extremely positive view, and a three- or four- star rating reflects a moderate assessment (Mudambi & Schuff, 2010). Some star ratings reflect extreme attitudes toward a product or service, as indicated by a deviation from the midpoint of the attitude scale (Krosnick, Boninger, Chuang, Berent, & Carnot, 1993). For example, a three-star review can reflect a moderate view, which can indicate indifference, but it can also comprise a series of positive and negative comments that cancel each other out, which indicates ambivalence.
The kind of reviewer who likes to give extreme ratings is generally unknown. In this study, we define extreme ratings as one-, two-, and five-star ratings. A question that should be addressed is whether the badge levels and the travel experience that these ratings represent are considered important by reviewers. For exam- ple, experienced and inexperienced travelers differ in many aspects (Yoo & Gretzel, 2008). Experienced travelers know what to expect when visiting a hotel or restaurant, and they understand what a place can or cannot deliver, or what is within acceptable limits and what is not (Jeong & Jang, 2011). Furthermore, experi- enced travelers may be more patient than inex- perienced ones and are also likely to be more professional when giving ratings and posting reviews (Shanteau, Weiss, Thomas, & Pounds, 2002; Willemsen, Neijens, & Bronner, 2012). Inexperienced travelers tend to be overcritical because they do not understand the industry well, have not formed a realistic set of expecta- tions, and do not know who they should hold accountable if something goes wrong (Black & Kelley, 2009). Therefore, inexperienced and experienced reviewers act differently when they rate services or products. Experienced reviewers with a higher status are more objec- tive and unbiased, whereas inexperienced reviewers are harder to please and seek status (Kim, Mattila, & Baloglu, 2011). This finding can be related to the findings of Kemper (1991) or Merton (1968), where high-status and experi- ence-rich individuals tend to be more (posi- tively) balanced than (aggressive) status seekers. According to this analysis, we propose the first hypothesis (H).
H1: Reviewers with high-level badges will be more likely to give moderate than extreme ratings.
Schuckert, Liu, and Law 5
2.3.
Helpful Votes and Value Creation
As noted earlier, we know that online plat- forms such as Amazon, eLong, and Qunar allow readers to give helpful votes to contribu- tions or comments made by reviewers. This kind of feedback assures the quality and cred- ibility of posts, as well as constituting a sec- ond-level source of support for potential customers (readers are more likely to rely on the customer reviews that are marked with more helpful votes). Readers rate reviews as helpful when they consider the provided infor- mation to have been useful to them. This situa- tion can take place before a trip when customers are looking for potential accommo- dation. The readers vote by clicking on an icon. The total number of clicks represents the value of the review, which means that a high number of votes can indicate greater help- fulness (Lee et al., 2011).
This finding raises the question of what makes an online review helpful and valuable. Pavlou and Dimoka (2006) show that extreme ratings are more influential than moderate ones on eBay. Forman, Ghose, and Wiesenfeld (2008) similarly show that, for books, moder- ate reviews are considered less helpful than extreme ones. However, Baek, Ahn, and Choi (2012) found the opposite to be the case in their study of Amazon reviews, which showed that a greater difference between review star rating and product average rating corresponds to a lower helpfulness score. They report that higher star and product average ratings result in fewer helpfulness votes. Sen and Lerman (2007) explain this apparent contradiction by suggesting that the degree of helpfulness depends on product type regardless of the posi- tive or negative tone of the actual review. Mudambi and Schuff (2010) show that product type controls the effect of an extreme review on the helpfulness score, given that the readers who view the products have different

6 JOURNAL OF TRAVEL & TOURISM MARKETING
information needs. Therefore, not all low rat- ings or negative reviews will meet the readers’ requirements and produce helpfulness votes. Ultimately, the helpfulness of a review is related to product type.
Studies also show that reviewer attribution can affect the perceived helpfulness of reviews. According to Forman et al. (2008), reviews that disclose identity-descriptive information about the reviewer are rated as more helpful than anonymous ones. Meanwhile, Lee et al. (2011) indicate that the travel frequency and ratings of reviewers can also affect the helpfulness score that they give to their reviews. In this manner, the reviewer badge level, which represents experience, authority, and status, is an important reviewer attribute. However, as mentioned ear- lier, research on the effect of reviewer badge level on value creation remains lacking. Based on the results of these studies, we ask whether reviewers with higher-level badges generate more value and obtain a higher proportion of helpfulness votes than reviewers with lower- level badges.
We infer that a reviewer with a high-level badge will contribute less than one with a lower status for several reasons. Firstly, reviewers who already have a high-level badge will be less motivated to contribute a review. When someone initially joins such websites, he or she may aim to collect badges to obtain more recognition from readers, in a manner similar to playing online games. However, once a reviewer reaches the highest badge level, he or she may no longer be motivated to participate. The quality or value of their reviews may decline, which results in decreasing helpfulness votes. Secondly, badges do not only represent experience, but also status on the site where they appear. After a reviewer has achieved this status, he or she may become less competitive than other users (Antin & Churchill, 2011). Based on this analysis, we propose our second hypothesis.
H2: Reviewers with high-level badges will post fewer helpful reviews and receive a lower proportion of helpful votes than reviewers with low-level badges.
3. METHODOLOGY 3.1. Data and Variables
TripAdvisor is the best-known platform for online reviews in the hospitality industry and is increasingly considered as a metric for the quality of hotels and other tourism-related services. In early 2013, TripAdvisor had over 200 million monthly unique users, according to Google Analytics (Lee et al., 2011). However, we did not include all users in our analysis because of the considerable amount of time and effort required to process such data. Instead, we selected reviewers from different regions and hotel classes to ensure that a suitably unbiased sample was chosen (Ye, Law, Gu, & Chen, 2011). We chose hotels from all classifications ranging from one star to five star in Hong Kong, listed on TripAdvisor, collected information on all reviewers associated with these hotels, and omitted duplicates. This approach ensured both the size and validity of the sample. Hong Kong is an international city that attracts travelers from all over the world; hence, the reviewer sample is diverse. Using a crawler, selected web page related data were downloaded that show reviewer badges, review ratings, and helpful votes (Ye, Zhang, & Law, 2009). In addition, another Java-based program was developed to parse HTML- and XML-web pages into our database. Data col- lection was conducted in August 2013. The crawler only requires several seconds to col- lect data on each reviewer. Thus, the status of each reviewer will not change during the data collection period. The actual benefits (mone- tary rewards or coupons) that users obtain from various platforms differ. For example, Qunar, Ctrip, and eLong offer points or cou- pons that can be used by people who post reviews, whereas Booking.com provides dis- counts. Platforms like TripAdvisor, however, do not offer any kind of tangible rewards (monetary rewards or coupons). TripAdvisor for example, only offers badges to reviewers. This practice is the main reason why TripAdvisor was chosen for analysis in this study because the effect of an actual monetary

or quasi-monetary benefit on reviewer beha- vior can be excluded.
Each review has a corresponding number of helpful votes. If readers consider a review to be helpful, then they can vote for it; if a review is not considered to be helpful by any reader, then it will receive no vote. We collected the number of helpful votes received and the number of ratings posted by each reviewer, including excellent, very good, average, poor, and terrible. As discussed earlier, the extremeness and help- fulness of a review can be measured by its rating and helpful votes. However, the review content itself can also be a factor. Thus, at least two approaches are available to measure the extremeness and helpfulness of a review, namely the score and the content. Text mining (sentiment analysis) of review content can clas- sify online reviews into positive or negative, or helpful or unhelpful, and has been a popular research topic over the past few years (Ye, Zhang, et al., 2009; Zhang, Ye, Zhang, & Li, 2011). However, the accuracy rate is only approximately 70% and the biggest challenge is conducting such an analysis in different lan- guages, which will further reduce accuracy rate. Consequently, this study used ratings rather than content to measure review extremeness and helpfulness. The variables in our research are defined in Table 1.
3.2. Research Design
Three steps are employed in our research. Firstly, we transformed TripAdvisor badge levels into their equivalent numerical values (from 1 to 5) to conduct the empirical study (Table 2). We excluded the 0 level. According to TripAdvisor (see Figure 1), a reviewer receives no badge if he or she posts fewer than three reviews. For exam- ple, if a reviewer posted 0/1/2 reviews, then the corresponding badge level would be 0. These reviewers were excluded because an extremely small number of reviews is inappropriate or insufficient to calculate variable ratio (excellent, very good, average, poor, terrible). If a reviewer has only posted one review, then the “excellent” ratio will be 1/1 or 100%, whereas other ratios such as “very good” and “average” will be 0. This situation will lead to bias because ratio or the probability of each kind of score (from excel- lent to terrible) should not be calculated using only a few reviews (that is fewer than 3). Given that we cannot measure the satisfaction degree of a hotel by using only one or two ratings, we exclude hotels with fewer than 10 ratings to ensure that bias will not be introduced (Liu, Schuckert, & Law, 2014; Schuckert, Liu, & Law, 2014).
Secondly, we computed every variable at the reviewer level rather than at the review level.
 Variables
Badges Hratio Excellent Very Good Average Poor Terrible
Badges
TABLE 1. Variables Used for Analysis Description
Badge levels, based on the badges given by TripAdvisor (see Table 2)
The number of helpfulness votes/The total number of ratings for each reviewer
Excellent ratio: The number of “excellent” ratings/The total number of ratings for each reviewer Very good ratio: The number of “very good” ratings/The total number of ratings for each reviewer Average ratio: The number of “average” ratings/The total number of ratings for each reviewer Poor ratio: The number of “poor” ratings/The total number of ratings for each reviewer
Terrible ratio: The number of “terrible” ratings/The total number of ratings for each reviewer
TABLE 2. Numerical Definitions of Badge Levels
Reviewer Senior reviewer Contributor Senior contributor Top contributor
Schuckert, Liu, and Law 7
        Levels 1 2 3 4 5
 
8 JOURNAL OF TRAVEL & TOURISM MARKETING
The reviewers for TripAdvisor can post three kinds of online review/rating: hotels, restau- rants, and attractions. TripAdvisor does not dif- ferentiate between a review of a hotel, a restaurant, or an attraction when calculating the number of reviews a reviewer has posted. Thus, the effect of badges on reviewer behavior should be consistent with the badges arranged by TripAdvisor. In this study, we collected data on badges based on the total number of reviews rather than on the number of hotel reviews. When we calculate the extremeness ratio (Eratio) and the helpfulness ratio (Hratio), we also used all the reviews or ratings posted by a reviewer. For example, the Hratio was com- puted as follows. If a reviewer posted 10 reviews in total and gained three “helpful” votes, the Hratio of this reviewer was computed as 3/10 = 0.3, and so on for other variables. Thirdly, we computed the ratio of each kind of rating (excellent, very good, average, poor, and terrible) for each reviewer. We then ran a corre- lation analysis between reviewer badge levels and each of the rating level to explore our hypotheses.
TABLE 3. Descriptive Statistics of Variables
    Badges Hratio Excellent Very good Average Poor Terrible
Minimum Maximum Mean Standard
1.00 5.00 3.031 1.317 0.00 10.00 1.002 1.088 0.00 1.00 0.399 0.233 0.00 1.00 0.364 0.194 0.00 1.00 0.155 0.132 0.00 1.00 0.052 0.079 0.00 1.00 0.030 0.069
N
43,764 43,764 43,764 43,764 43,764 43,764 43,764
 4. 4.1.
RESULTS AND DISCUSSION Descriptive Statistics
Among the reviewers, 15.5% regarded their experience as “just okay”, whereas only a small proportion (8.2%) rated their satisfaction as “poor” or “terrible”.
4.2. Rating Behavior
Firstly, we analyzed the relationship between reviewer badge levels and ratings (1–5). A sig- nificant negative relationship exists between badge level and “excellent”/“poor”/“terrible” ratings, whereas a positive relationship exists between badge level and “very good”/“average” ratings (Table 4). This finding indicates that reviewers with high-level badges are more cau- tious than reviewers with low-level badges, and thus are less likely to give extreme ratings such as 1, 2, or 5. In most cases, they tend to give relatively moderate scores such as 3 and 4.
Secondly, as defined in H1, we conducted a correlation analysis between reviewer badge level and review extremeness. We measured the latter in two ways (scores of 1 and 5 and scores of 1, 2, and 5) because a rating of 2 is considered as extremely negative, and only a few reviewers give ratings of 1 or 2. Badge level is significantly negatively correlated with extreme ratings regardless of how the latter are defined (Table 5). In this analysis, an extreme review score is computed for each reviewer and the measure of extremeness is equal to the pro- portions of 1, 2, and 5, or the scores of 1 and 5.
Notably, reviewers with high-level badges tend to post reviews and ratings in a cautious and responsible manner. We suggest two possi- ble reasons why such reviewers are more responsible and objective than reviewers with a lower status. Firstly, they are aware that their
Data for 43,764 reviewers were selected and deemed valid after omitting those with missing values (Table 3). In addition, 10,451 reviewers (who posted fewer than three reviews) have no badge. The total number of reviews posted by all reviewers is 1,181,935. Badge levels ranged from 1 to 5, with an average value of 3. By contrast, the range of the Hratio was significantly wide, that is, from 0 to 10, with an average value of 1. This finding indicates that the average reviewer receives only one “helpful” vote (for all their reviews, rather than just one). A reviewer can give ratings from 1 to 5. According to the descriptive statistics, most reviewers tended to give “excellent” and “very good” ratings, with an average of 76.3%. This result suggests that most travelers were satisfied with the hotels or restaurants that they had visited.

Schuckert, Liu, and Law
9
TABLE 4. Correlation Analysis
   Badge Hratio Excellent
Very good
Average
1.000
Poor
       Badge 1.000
Hratio −0.160** 1.000 Excellent −0.126** −0.006 Very good 0.113** −0.059** Average 0.134** −0.011* Poor −0.031** 0.093** Terrible −0.114** 0.102**
1.000 −0.692** −0.539** −0.252** −0.106**
Terrible
1.000 at 0.01 level (2-tailed); *correlation is significant at 0.05 level
TABLE 5. Badge Level and Extreme Review Score Correlation Analysis
contribution to readers declines. Hence, H2 is supported.
This phenomenon has many potential expla- nations. The most direct explanation is that readers prefer reviews with low ratings, but reviewers with high-level badges tend to post more moderate reviews and avoid very high or low ratings. A significant positive relationship exists between Hratio and the scores of 1 and 2, whereas a negative relationship exists between Hratio and the scores of 3 and 4 (see Table 4, second row). By contrast, reviews with low ratings (1 and 2) receive more “helpful” votes. This result indicates that readers find bad reviews (namely those with low ratings) more helpful, and thus vote for them accordingly. In addition, reviews may also be read to assist in making travel decisions, and most potential cus- tomers try to avoid risks by focusing on bad reviews. When readers find several bad reviews, they may postpone their travel plans or consider another hotel. Hence, excellent (insignificant) reviews may be completely ignored (Lee, Park, & Han, 2008).
Another possible explanation is that the qual- ity of reviews is reduced as badge status rises. In order to explore the relationship between reviewer badge level and extreme review scores and value creation, we computed Eratio and Hratio according to reviewer badge levels (1– 5). Eratio is defined as the proportion of 1 and 5 scores respectively of each reviewer, with the average value computed according to reviewer badge levels; the same procedure is applied to Hratio (Table 6). As shown in Figure 2, Eratio is reduced as badge status rises, which indicates
1.000 −0.030** −0.165** −0.228**
0.038** −0.058**
1.000 0.095**
 Notes. Hratio: helpfulness (2-tailed).
ratio; **Correlation is significant
 Extreme review score
(Scores 1, 2, and 5) (Scores 1 and 5)
Reviewer badge level
−0.171** −0.158**
  Notes. **Correlation is significant at 0.01 level (2-tailed).
reviews or ratings will influence more travelers; hence, they rate carefully and avoid any kind of extreme reaction (too positive or too negative). Secondly, they are more experienced and have traveled more, and thus they can make better comparisons and offer more objective ratings. Therefore, H1 is supported.
4.3. Value Creation
We also examined whether reviewers with high-level badges produce more helpful and valuable information and whether the quality or value of a review declines as a reviewer posts more reviews. We therefore conducted a correlation analysis between reviewer badge level and Hratio to explore these issues.
A significant negative relationship exists between Hratio and reviewer badge level (coef- ficient: −0.160; significance level: 0.01) (see Table 4). This finding indicates that the review quality of reviewers with high-level badges, who are highly productive (they post more reviews) decreases, and thus these reviews receive fewer “helpful” votes. That is, although such reviewers post more, the value of their

10 JOURNAL OF TRAVEL & TOURISM MARKETING
TABLE 6. Badge Levels, Hratio, and Eratio
travel experience and online badges on reviewer behavior (review extremeness and value crea- tion), which can help to improve our under- standing of online reviews. It also contributes to status-seeking theory in an online environ- ment. Online reviewers on OTA platforms can be motivated by badge levels to increase their status in their network life. This status-seeking behavior has changed the rating behavior and value creation of online reviewers. Two impor- tant insights can be gained from our empirical findings. Firstly, reviewers with high-level badges dislike giving extreme ratings and tend to give relatively moderate ratings. Secondly, although such reviewers produce more content, the quality of their reviews decreases and is considered to contribute less to readers. In addi- tion, readers find reviews with low ratings to be more helpful than those with high ratings, and thus are more likely to give “helpful” votes to the former within the context of the travel and hotel industries.
Based on these findings, we suggest that applications or platforms such as TripAdvisor and other OTAs must establish a new index to measure the quality of reviews, given that exist- ing indices focus only on quantity (even “help- ful” votes, which represent review quality, are measured by an absolute index rather than a ratio). A quality index may encourage reviewers to continue creating valuable contents, which will benefit the sustainable development of rat- ing platforms. In addition, social hierarchy also works in the online environment. That is to say, high-status reviewers create more careful and responsible reports, although readers are more anxious to learn about the negative aspects of a product or service.
Our research has several limitations. Firstly, we did not analyze the personal characteristics of the reviewers, even though background vari- ables can significantly affect online behavior. Gender, age, and other personal information were missing from our data set because they are regarded as private by most TripAdvisor members. Future research may consider the per- sonal characteristics of reviewers when explor- ing their behavior. Secondly, the correlation coefficients generated in our analysis were insufficient, and thus the same application can
Badge levels Hratio
1 1.316 2 1.111 3 0.975 4 0.867 5 0.785
Eratio
0.485 0.454 0.435 0.406 0.367
Notes. Hratio: helpfulness ratio; Eratio: extremeness ratio.
FIGURE 2. Helpfulness ratio (Hratio), Extremeness ratio (Ertio), and Badge Levels
1.4 1.2 1 0.8 0.6 0.4 0.2 0
Hratio Eratio
12345
Badge levels
that reviewers with a higher status are less likely to give extreme ratings. Hratio is also reduced when badge status rises, which suggests that reviewers with high-level badges produce less valuable reviews. Such reviewers may care less about quality once they have reached a rela- tively high status compared with when they were still working to gain status (through badge level). Current e-commerce platforms only offer the absolute quantity of “helpful” votes. Reviewers with high-level badges have already gained numerous votes and do not need to work for one or two more. However, if plat- forms supply an index that reflects the contribu- tion rate of a reviewer, which is a relative value, the situation may considerably change and reviewers may be inspired to post more valu- able content in the long run.
5. CONCLUSION
To our knowledge, this study is one of the first empirical ones to focus on the effects of
     Ratio
 
be used on different platforms such as Ctrip and eLong to further verify the proposed hypoth- eses. Thirdly, different platforms have various restrictions on posting reviews. For example, Agoda, HotelClub.com, Ctrip, and eLong only allow customers who have made bookings to post reviews and ratings. Other platforms such as TripAdvisor and Qunar allow users to post reviews by asking them to declare that the review is based on actual experience. This con- dition can influence the robustness of our results. When studying online reviews, the prac- tice of assuming that real reviews exist (reviews posted by customers with actual bookings) is more relaxed. The number of fake reviews may be minimal, and thus can only slightly influence the findings of this study. Fourthly, online rat- ings are a necessary but insufficient proxy to measure the opinion of reviewers. Therefore, future studies could combine content and rating to measure the opinion of customers or to explore consistency or heterogeneity between them. Lastly, given that a proportion of wrong clicks on the “helpful” vote button occur, a bug is possibly present in TripAdvisor and the valid- ity of the number of “helpful” votes is reduced. Users should therefore be reminded that they are casting their vote when clicking this button.
In this study, we only collected a cross-sec- tion data set of each reviewer and explored various online behaviors among reviewers with different badge levels. A future research direc- tion will be using a panel data set to search for dynamic changes in the rating behavior of reviewers.




In the age of e-commerce, every industry is involved in online sales, and the hospitality and tourism industry is no exception. According to current figures, 71% of independent travel- related bookings are done online, while 36% of all package tours are booked online. Many travelers have booked a room, made an airline ticket reservation, or reserved a table at a res- taurant online in recent months. Hotels, be they luxury or best value, use online travel agents
(OTAs) or booking platforms, and online sales form the biggest part of their revenue (Buhalis & Law, 2008). After consumption, customers give feedback online and such online reviews have become increasingly important. They are fast, up-to-date, and available everywhere, and have become the word-of-mouth of the digital age (Kaplan & Haenlein, 2010). From the per- spective of potential customers, these reviews are considered to be authentic, trustworthy, and helpful; they are also influential (Li & Hitt, 2008). Thus, online reviews play a critical role
 Markus Schuckert is Assistant Professor in School of Hotel & Tourism Management, The Hong Kong Polytechnic University, 17 Science Museum Road, TST-East, Kowloon, Hong Kong SAR (E-mail: markus.schuckert@polyu.edu.hk).
Xianwei Liu is PhD Candidate in School of Management, Harbin Institute of Technology, 92 Xidazhi Street, Harbin, 150001, PR China. (E-mail: liuxianwei@126.com).
Rob Law is Professor in School of Hotel & Tourism Management, The Hong Kong Polytechnic University, 17 Science Museum Road, TST-East, Kowloon, Hong Kong SAR (E-mail: hmroblaw@polyu.edu.hk).
Address correspondence to: Markus Schuckert at the above address.
Acknowledgement: The work described in this paper was supported by a grant funded by The Hong Kong Polytechnic University (A-PM08).
608
in the online sales of the hospitality and tourism industry, which mainly offers services and focuses on customer satisfaction. This is also the reason why potential customers spend so much time reading online reviews to assist their decision making (Zhu & Zhang, 2010). Customers like to search for objective opinions, and they prefer reviews, which are mostly delivered through large feedback platforms and consumer-centric sites, because of their inde- pendence from official or corporate content (Forman, Ghose, & Wiesenfeld, 2008).
Online reviews can be placed indifferent ways and generate powerful word-of-mouth online (Chen & Xie, 2008). A good reputation can bring a price premium, which expands the gap between online sellers and producers (Ye, Law, Gu, & Chen, 2011). The competition among online booking platforms has become so fierce that every online seller tries its best to attract potential customers. For example, an online travel agency’s high credibility and security assurance contribute to online bookings. Online buyers are less likely to be worried about fraud when making an online booking with a reputable platform such as TripAdvisor. This is probably because the online reviews platform plays an important role, attracting more travelers to post and share good or bad experiences (Zhong, Leung, Law, & Wu, 2013). This voluntary behavior of customers not only provides free information to assist the deci- sion-making process of other travelers but also encourages hospitality managers to improve their product or service quality.
This flourishing and influential application of e-commerce in hospitality and tourism has attracted academic attention since 2004 (Lee & Hu, 2004). As such, it is time to conduct a review of the research papers on hospitality and tourism online reviews, both for the aca- demic and practical value of such an exercise, and to indicate future research opportunities. In this study, we review the academic output in this field from the first relevant paper published in 2004 to the present. To date, although the role of social media in hospitality and tourism has been widely discussed, the relative impact of each type of social media has not been exam- ined in the extant research (Leung, Law, van Hoof, & Buhalis, 2013). To fill the gap, this
study reviews targets on academic output related to online reviews, which is the most influential and popular component of social media in hospitality and tourism (Buhalis & Law, 2008; Gu & Ye, 2014). Thus, this research is original and employs two approaches. First, we review all the scientific literature related to online reviews in order to form an overview and systematic order of the topics that have emerged, the findings that have been made, and the methods that have been applied. Second, we analyze the limitations of previous research and propose directions for future research in the field, providing implications for both fellow researchers and the industry. The purpose of this study is to deliver an advanced understanding of the development of hospitality and tourism online review research and an over- all perspective for future research efforts.
2. METHOD
The first step in this study was to select the databases from which to retrieve data. The data retrieval was conducted in September 2013, and repeated in March 2014, on Science Direct, EBSCOHOST, and Google Scholar, the three largest and most popular online databases/ search engines. In addition, references were traced in order to discover cross citations in the published articles (Law, Qi, & Buhalis, 2010). Second, keywords for data screening were identified following Leung et al. (2013). The keywords of online reviews, and the terms “hotel,” “restaurant,” “destination,” “hospital- ity,” “travel,” and “tourism” were used to search for online review-related articles published in academic journals. As the number of related articles was small, related papers from hospital- ity and tourism journals as well as journals from other disciplines were also gathered. In the third step, 50 articles published between 2004 and 2013 were determined to be relevant to this study after three rounds of data retrieval and screening. During this identification process, keywords and the abstract of each publication were analyzed to determine whether it was related to the focus of this study. The authors then reviewed the articles that had passed the
Schuckert, Liu, and Law 609

610 JOURNAL OF TRAVEL & TOURISM MARKETING
selection process. It is possible that personal bias may have affected the process. Therefore, the consensus of multiple authors, all of whom are experienced researchers in e-commerce, should have acted as a safeguard to minimize that possibility (Law, Leung, Au, & Lee, 2013). The next step was to identify the topical focus of each article. Following the analytical frame- work of Line and Runyan (2012), we adopted content analysis to examine the articles indivi- dually in terms of topical focus, target industry, and methodology applied. As this is the first review paper based on hospitality and tourism online reviews, no prior categories could be adopted. In a couple of cases, different results were encountered during the grouping process. Consensus on these issues was reached through in-depth discussion and analysis.
After the topical review, the authors analyzed the context in which the research was applied. This analysis not only helped in understanding the type of industry that the researchers were interested in but also identified the flourishing sector in terms of e-commerce application. The methodological review included an analysis and coding of three components: the sample source, the type of data, and the main analysis method used in each study. Further, the number of authors of each article and the number of related articles published in each journal were calcu- lated. From the timeline, we found the number of papers increased to reach a peak in 2011. Since then the numbers dropped, which may be the result of a trend of popularity and diver- sity in this research field (see Figure 1).
3. TOPICAL REVIEW
Technically, the early millennium marks the beginning of online travel review platforms. Researchers were attracted by this development, especially after the founding of Expedia and the introduction of TripAdvisor. Since then, 50 related papers have been published, which can be grouped into five topical clusters based on a content analysis. These articles are related to:
(i) online reviews and online buying, (ii) satisfaction and management,
(iii) opinion mining/sentiment analysis, (iv) motivation, and
(v) the role of reviews.
Table 1 provides the quantities of studies coded into each topic as well as the percentage of the total that each component represents. The most researched fields are online reviews and online buying (13 instances) and satisfaction and management (13 instances); these two cate- gories represent 52% of the total number of articles analyzed. Opinion mining/sentiment ana- lysis (n = 8), motivation (n = 8), and the role of reviews (n = 8) received the same level of atten- tion from hospitality and tourism researchers.
Table 2 summarizes the sectors that have attracted the interest of researchers, a sector being the subpart of the tourism and hospitality industry for which the major implications of a study are intended (Line & Runyan, 2012). The sectors are defined according to the categories identified by Oh, Kim, and Shin (2004). One can
FIGURE 1. Trends of Related Publications of Hospitality Review-Related Articles
14 12 10
8 6 4 2 0
2004 2005 2006 2007 2008 2009 2010 2011 2012 2013
Year
    Number

Cluster
Online reviews and online buying
Satisfaction and management Opinion mining/
sentiment analysis Motivation
The role of reviews
TABLE 1. Topical Clusters
Cluster description
The effect of online reviews (the number of reviews, the valence of reviews, and the ratings) on consumer purchase intention, commodity price, and online sales.
Customer satisfaction and online complaints, and managers’ online management.
The valence of online reviews (positive or negative), product/service feature extract, content
analysis (word frequency).
The motivation or the purpose of posting, reading, and sharing online reviews. The credibility and helpfulness of online reviews on e-commerce platforms.
N % 13 26 13 26
8 16
8 16 8 16
TABLE 2. Analysis of Industry Sectors
online reviews posted by other customers who have relevant experience. Here, online reviews are very important to both sellers and buyers, and so researchers focus on three aspects of online reviews: their effect on purchase intention, price, and sales.
The valence (positive or negative) of online reviews and ratings has a significant impact on potential consumers and their purchase deci- sions. The results show a positive correlation between hotel purchasing intention and the valence of reviews (Mauri & Minazzi, 2013; Sparks & Browning, 2011). Furthermore, high ratings can generate customer revisit intention (Miao, Kuo, & Lee, 2011; Zhang & Mao, 2012). Vermeulen and Seegers (2009) con- ducted an experimental study and found that positive reviews have stronger effects on les- ser-known hotels. Zhang, Ye, Law, and Li (2010) and Zhang, Zhang, Wang, Law, and Li (2013) found a similar relationship between rat- ings of restaurants and purchase intention.
The number of reviews and the proportion of good reviews, especially the ratings, can have a great impact on hotel rates and restaurant prices as well as on related online sales. High ratings and positive word-of-mouth (WOM) can gener- ate a price premium for hotels (Yacouel & Fleischer, 2012; Zhang, Ye, & Law, 2011). Öğüt and Onur Taş (2012) found that a higher customer rating significantly increases the online sales of hotels and that a 1% increase in online customer ratings increases sales per room by up to about 2.6% depending on the destination. Ye, Law, and Gu (2009) as well as Ye et al. (2011) found that traveler reviews have a significant impact on online sales, with a 10%
Schuckert, Liu, and Law 611
     Sectors
Hotel/Lodging Travel/Tour Restaurant/Foodservice Others
N %
30 60 9 18 9 18 2 4
  clearly see that the hotel industry attracts the attention of most researchers (n = 30), account- ing for 60% of all the papers examined. The travel (n = 9) and restaurant (n = 9) sectors are of relatively less concern, each being the focus of only 18% of the papers. In addition, two articles focus on the whole perspective of the tourism and hospitality industry. The dominant position of hotels in hospitality and the overwhelming application of e-commerce have led to the boom- ing development of the hotel industry, and this has attracted the attention of an increasing num- ber of academics and practitioners. In addition, along the tourism value chain, reviews are most applicable for the hospitality sector.
3.1. Online Reviews and Online Buying
The development of e-commerce has dramati- cally changed the way that customers search for information about, make their choice regarding, and purchase products or services (Zhu & Zhang, 2010). Customers are driven by questions such as “How can I find the best attraction?,” “Where is the best restaurant?,” or “Is a comfortable hotel really worth the money?” When customers try to find answers to these questions, they consult

612 JOURNAL OF TRAVEL & TOURISM MARKETING
increase in traveler review ratings boosting online bookings by more than 5%. Lu, Ba, Huang, and Feng (2013) obtained comparable results for the restaurant industry based on the restaurant reviews and online orders.
Ignoring uncontrollable variables such as fake reviews and reviews’ reliability, the above studies still have some limitations. First, most prior research has been conducted using the number of reviews as a proxy vari- able representing the real online sales of a hotel or restaurant. This is a very simplistic hypothesis and, in our view, has limited valid- ity. Second, the results are limited in terms of sampling process and sample size. For exam- ple, some studies only used hotels in one city (e.g. Lu et al. (2013) only focused on restaurants in Shanghai), and the other studies focused on a few hundred respondents (e.g., Mauri & Minazzi (2013) only used 349 respondents). Small and biased samples may affect the conclusions of an empirical analysis and the generalized meaning of findings.
3.2. Satisfaction and Management
For consumers, an online review is the most common way to place a complaint, express their feelings, comment on their satisfaction, and to rate a place, service, or hotel. Turning to the supply side and to managers, online reviews represent the best channel through which to assess additional information on service deliv- ery, quality, and customers’ demand, specifi- cally from negative reviews with low ratings as they are more likely to reflect real problems. The retrieval and analysis of this valuable infor- mation can thus significantly help managers’ self-improvement (Pantelidis, 2010). It is criti- cal for a hotel or restaurant to know about customers’ dissatisfaction. The words used in high satisfaction reviews are different from those used in low satisfaction reviews. In high rating reviews, reviewers prefer certain combi- nations of words such as “staff,” “clean,” “breakfast,” while “dirt,” “bed,” and “bath- room” appear more often in low-rating reviews (Levy, Duan, & Boo, 2013; O’Connor, 2010; Stringam & Gerdes, 2010). Empirical findings
reveal that price has a more significant impact on perceived satisfaction for upper-class luxury hotels than for lower-star hotels (Hui, Law, & Ye, 2009; Ye, Li, Wang, & Law, 2012). Jeong and Jeon (2008) noted that value is one of the key predictors for guest satisfaction, which leads to increased return intentions regardless of the location and hotel class. In addition, offering free Wi-Fi works well in terms of sig- nificantly improving guest satisfaction levels (Bulchand-Gidumal, Melián-González, & López-Valcárcel, 2011).
Online response management thus becomes more important. Managers who respond suc- cessfully to comments on electronic platforms can turn an unsatisfied customer into a loyal one (Pantelidis, 2010), while appropriate responses to reviews, especially bad reviews, can create and increase future revenue (Noone, McGuire, & Rohlfs, 2011). Meanwhile, in marketing management, eWOM ranks as the most impor- tant information source when a consumer is making a purchase decision (Litvin, Goldsmith, & Pan, 2008; Shaw, Bailey, & Williams, 2011). However, it is far from easy to generate good results simply through managing or responding to online reviews. Empirical research shows that although online responses increase the future satisfaction of complaining customers, the future satisfaction of complaining custo- mers who do not receive responses decreases (Gu & Ye, 2014). Therefore, the question of how to improve satisfaction levels, increase revisit intention, and use appropriate strategies in responding to customers’ complaints need to be discussed further.
The limitations of the above research are mainly related to two points. First, we may not know whether online reviews are telling the truth because sometimes a good review does not represent high satisfaction. Some reviewers tend to give a good review to avoid unnecessary trouble even though they have an unpleasant experience. Racherla, Connolly, and Christodoulidou (2013) found that the correla- tion between overall rating and ratings on individual attributes is very low, suggesting that the overall numerical ratings typically used in review systems may not be the ideal indicators of customers’ perceived service

quality and satisfaction. The validity of using online reviews or ratings to measure customer satisfaction levels thus needs to be explored more closely in the future. Second, it is hard to scale, rate, or measure the effect of review management and the return on such efforts; increasing sales may be the result of a seaso- nal effect, spill-over effects at a destination, or other hidden aspects. The efficiency of managers’ different response strategies should be studied further. For example, it is impor- tant to find out whether responding to all reviews is more fruitful and results in higher satisfaction levels than responding only to selected complaints or even not responding.
3.3. Opinion Mining/Sentiment Analysis
Given the sheer number of online reviews, there is a definite need to understand what a review focuses on and whether the content is positive or negative in nature. This is a rather technical exercise and is related to big data analysis. The processes are heavily dependent on algorithms and programming as well as com- puter processing speed. Artificial judgment has been proven time consuming, exhausting, and costly. Here, a way forward is the use of artifi- cial intelligence and the opinion mining techni- que (Akehurst, 2009). Generally speaking, opinion mining has two applications: valence and feature extraction.
Valence means dividing reviews into positive and negative. Commonly, three supervised machine learning algorithms (Naive Bayes, Support Vector Machine [SVM], and the char- acter-based N-gram model) are applied to deal with the sentiment classification of online reviews. Empirical findings have indicated that the SVM and N-gram approaches outperform the Naive Bayes approach (Ye, Zhang, & Law, 2009). In order to improve accuracy, Kang, Yoo, and Han (2012) proposed an improved Naive Bayes algorithm for sentiment analysis. Zhang, Ye, Zhang and Li (2011) also applied sentiment analysis to restaurant reviews written in Cantonese.
Feature extraction means extracting related information about a product or service as well
as the topic, keywords, and concerns of the customers from a massive number of reviews. The aims of opinion mining of e-complaints are to find the most unhappy experience from cus- tomer feedback (Lee & Hu, 2004) and to mine word patterns using semantic clustering of con- sumer opinions from a large amount of qualita- tive data retrieved from online travel reviews (Capriello, Mason, Davis, & Crotts, 2013). Pekar and Ou (2008) investigated a method to recognize the relationships between subjective expressions and references to certain features of a product, while Stringam, Gerdes, and Vanleeuwen (2010) and Stringam and Gerdes (2010) studied the willingness of reviewers to recommend a hotel based on the mining of online rating data.
Studies on opinion mining face some lim- itations. Firstly, precision is still limited because the sentiment classification that divides reviews into positive and negative reviews is only around 70% accurate (Zhang et al., 2011). Moreover, the technique’s ability to catch the real meaning of reviews is still considered not intelligent enough. Sometimes, the mining process returns just a collection of keywords which is far from any expected result. Secondly, hospitality is a global busi- ness and travelers come from all over the world, and it is difficult for opinion mining programs to handle different languages. Thus, mining options are valid only for the main and settled language of the routine: efficiency and accuracy may differ significantly from language to language. Thirdly, there is a lack of application and motivation. The academic research on opinion mining has come to a standstill in recent years for two reasons (Akehurst, 2009). The first is that the aca- demic findings have not been applied to prac- tice, either because they have limited practical value or because it is simply too costly to adopt such mining technology in daily busi- ness, an issue which is related to the uncertain return on investment of reputation manage- ment. Additionally, the lack of a practical application may in turn have diminished enthusiasm for academic research since this field is by its very nature practical and applied.
Schuckert, Liu, and Law 613

614 JOURNAL OF TRAVEL & TOURISM MARKETING
3.4. Motivation
Faced with the growing number of online reviews, TripAdvisor offers over 40 million tra- veler reviews to peruse and ranking lists for over 125,000 visitor attractions, 450,000 hotels, and 600,000 restaurants (Jeacle & Carter, 2011). It is valuable, not only from an industry view- point but also from an academic one, to find the answer to the questions “Who creates online reviews?” and “What is the motivation behind writing, seeking out, and sharing online reviews?” Here, the research stream is heading in the direction of consumers’ psychological behavior to discover the triggers and motiva- tions behind such activities. The empirical find- ings show that online travel reviewers are motivated by a desire to support the service provider by providing feedback and to push for improvements in service quality, as well as a concern for future consumers and their experi- ences (Yoo & Gretzel, 2008). In general, trave- ler personality can significantly influence perceived barriers to this feedback and content creation and the motivations to engage in con- sumer-generated media (CGM) creation beha- vior, while in general women are more motivated by the desire to help the company and men are motivated by the desire to prevent others from falling into traps (Yoo & Gretzel, 2011). Öğüta and Cezara (2012) found that higher rating and lower price increase the pro- pensity to write reviews that and complaints over bad experiences also motivate customers to write reviews, probably bad reviews. Sparks and Browning (2010) found that customers who experience service failure tend to spread nega- tive word-of-mouth and have the potential to influence a brand or firm’s reputation. However, the motivation for posting negative word-of-mouth differs from customer to custo- mer: for example, ranging from taking revenge to warning others (Wetzer, Zeelenberg, & Pieters, 2007). To explore the underlying moti- vations for and barriers to knowledge sharing and the spreading of positive reviews (Huang, Basu, & Hsu, 2010), empirical studies have been conducted in restaurants to show that a restaurant’s food quality positively influences customers to spread positive eWOM. In
addition, restaurant service employees who cre- ate a good experience can trigger positive eWOM motivated by the desire to support the employees and the restaurant or just to express personal positive feelings (Jeong & Jang, 2011). Furthermore, Kim, Mattila, and Baloglu (2011) found three major motivating factors for consu- mers to seek eWOM: convenience and quality, risk reduction, and social reassurance. Women are more likely to read reviews for the purpose of convenience, desired quality assurance, and risk reduction, while men’s use of online reviews depends on their level of expertise in online booking.
Among the articles analyzed, six are based on primary data comprising only hundreds of samples. Although questionnaire is the most direct way to obtain information on the beha- vior of customers, sample size should be large enough and constructed without bias in order to avoid limitations. As the motivation to post and share reviews stretches into psychology and consumer behavior, which is hard to observe and measure, the best and most efficient approach is an experiment or questionnaire through which the researcher can obtain first- hand data. However, this approach is also affected by the source and number of respon- dents. Respondents may come from the same region, and the samples may not be big enough for both data analysis and conclusion general- ization (Iacobucci, 2010).
3.5. The Role of Reviews
What role do online reviews play within our perception of reality? Online reviews work as a medium between customers and service provi- ders which can not only reflect satisfaction with the consumption experience but also offer valu- able information to help potential consumers make decisions (Bissell, 2012; Xiang & Gretzel, 2010). It is important (a) to be aware of the role of online reviews and to have stra- tegies in place to deal with this fast emerging medium (Dwivedi, Shibu, & Venkatesh, 2007), and (b) to understand how online reviews are used by consumers, the role they have in searching for information, and their impact on

travel behavior. All of these issues are and will continue to be important for the hospitality industry (Cox, Burgess, Sellitto, & Buultjens, 2009).
Credibility and helpfulness are very critical attributes of online reviews, especially hospital- ity reviews, which focus on customers’ feelings and experiences. The findings show that the use of online reviews is widespread and that trust depends on the type of website on which the reviews are posted as well as on the reader’s personality (Mkono, 2012; Yoo & Gretzel, 2010). Findings on the helpfulness of online reviews show that helpful reviews relate to the travel experiences of the customers who post them; experienced travelers actively post reviews and tend to give lower hotel ratings (Lee, Law, & Murphy, 2011). Moreover, con- sumers tend to give higher helpfulness scores to reviews that document an effective service recovery (Black & Kelley, 2009). Credibility and helpfulness are the most important aspects when potential consumers read online reviews to help them make decisions, and so consumers spend a lot of time checking whether a review is credible. Usually, bad reviews are rated as more valuable since they can help consumers avoid potential losses (Chevalier & Mayzlin, 2006).
Studies that focus on the role of reviews are usually more conceptual, inevitably lacking any attempt at empirical analysis, especially cause and effect analysis. Studies in this field usually use primary data such as questionnaires, which are costly in terms of human and financial resources. Thus, the sample size and the scope of such studies are sometimes limited. The “helpful” function has been widely used in practice as moste-commerce platforms allow readers to mark the “helpfulness” of a specific review. This function can generate valuable reviews (reviews marked helpful can play a bigger role in helping others) by readers, who are also potential customers searching for useful information to help make decisions (Mudambi & Schuff, 2010). While the credibility of online reviews is not very generalizable since it is hard to confirm whether a review is true and reflects the facts and reviews differ from individual to individual: for example, some guests may feel that a particular room is very spacious while
others may regard it as very small. Therefore, determining the credibility of online reviews is more complicated than expected.
4. METHODOLOGICAL REVIEW
Table 3 summarizes the results of the meth- odological analysis, including sample source, data type, and analysis method. In terms of sample source, 14 (28%) studies were obtained data from the United States, nine (18%) from China, four (8%) from the UK, 12 (24%) from international sources, and 11 (22%) used other sample sources. Of the studies that used inter- national data, the USA and China were the most popular sources, possibly due to their relatively developed hospitality and tourism industry or the brisk travel demand of these two large countries.
In regard to data type, 12 (24%) use primary data, 33 (66%) employ secondary data, and 10% do not use any data at all as they are primarily conceptual. More than half of the studies use secondary data since most of the research required trawling reviews from e-com- merce platforms and related websites in order to run an empirical or sentiment analysis. Primary data are mostly used for studying motivation
TABLE 3. Methodological Analysis N%
Sample source USA International China
Schuckert, Liu, and Law 615
  14 28 12 24 9 18 UK 48
Other
Data type Primary Secondary None Qualitative Quantitative None
Method
Empirical (Factor/Regression/ANOVA)
Content/Sentiment Analysis
Conceptual 4 8 Descriptive 4 8 Other 2 4
11 22
12 24 33 66 5 10 6 12 39 78 5 10
31 62 9 18
 
616 JOURNAL OF TRAVEL & TOURISM MARKETING
and satisfaction, whereas secondary data are mainly applied to investigating the impact of online reviews on sales. Articles that did not use any data focused on the role of reviews in travelers’ online behavior.
Finally, the methods applied for each article were analyzed. In general, quantitative studies were more dominant, representing 78% of all articles. Only 12% used a qualitative approach. Our analysis shows that empirical techniques such as factor analysis, multiple regression, or the analysis of variance, are the most frequently employed methods, present in more than 60% of the literature reviewed, followed by content/ sentiment analysis (18%), by applying machine learning techniques like Naive Bayes and SVM as well as content analysis. However, the use of conceptual and descriptive methods is very lim- ited. A data-driven pattern is the most common structure as many researchers prefer using sec- ondary data or applying empirical approaches to test or explore theoretical assumptions (Leung et al., 2013). Others used the method to develop
TABLE 4. Number of Authors
 Number of authors
N %
1 2 3 4 5
5 19 18 7 1 10 38 36 14 2
      Name of Journal
TABLE 5. Titles of Journals
n %
Hospitality Journal Yes/No
Y Y Y Y N Y Y N Y N N N Y Y Y N Y Y N N N N N 12/11
their theory and offer managerial implications based on their findings.
Among the 50 articles reviewed, 19 have two authors (38%), 18 have three (36%) and eight have more than four (16%). Only five papers have a single author (10%). Table 4 shows that multiple authorship is more common than single author- ship; the most common number of authors being two or three. multiple authorship seems to be a common practice in tourism and hospitality research in the interest of efficiency, research facil- itation, distribution of workload, and maximization of publication rewards (Law et al., 2013).
Table 5 shows the 23 journals that have published papers on online hospitality and
    Journal of Hospitality Marketing & Management International Journal of Hospitality Management Tourism Management
Cornell Hospitality Quarterly
Expert Systems with Applications
Journal of Travel & Tourism Marketing
Information Technology & Tourism
Computers in Human Behavior
International Journal of Contemporary Hospitality Management
Information Systems Research
International Journal of Cultural Studies
Journal of Business Research
Journal of Hospitality & Tourism Research
Journal of Hospitality and Tourism Technology
Journal of Quality Assurance in Hospitality & Tourism
Journal of Revenue & Pricing Management
Journal of Travel Research
Journal of Vacation Marketing
Procedia-Social and Behavioral Sciences
Productions and Operations Management
Psychology & Marketing
Service Business
Service Industries Journal
Total 50 100
Note: Y indicates that the journal is a hospitality/tourism journal, N indicates that it is not.
9 18 7 14 5 10 3 6 3 6 3 6 2 4 2 4 2 4 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2
 
tourism reviews. Twelve are hospitality and tourism journals. The distribution of the articles is heterogeneous. The most frequent journals are Journal of Hospitality Marketing & Management (n = 9), International Journal of Hospitality Management (n = 7), and Tourism Management (n = 5).
mainly by customers’ online reviews represents their core competitiveness by which to attract potential customers and secure more online sales. Online management, as a marketing strat- egy, was rapidly accepted and expanded over time. Leung, Law, and Lee (2011) found that more than half of the hotels studied in Hong Kong still had not adopted social media man- agement. According to the statistics at the end of 2013, however, more than 70% (102) of the 145 hotels in Hong Kong with more than 100 reviews on TripAdvisor had adopted online response management. The awareness and prac- tice of online management is the result of the most influential aspect of online reviews, espe- cially bad reviews that spread negative word-of- mouth. Poor reviews, on the one hand, lead to reputation damage for suppliers; on the other, they also offer opportunities for suppliers to make improvements.
Meanwhile, this study fills the gap indicated by Leung et al. (2013): that the extant literature does not examine the impact of each type of social media on travelers’ purchase decisions. Not only have we focused on the impact of online reviews on online purchases, but we have also classified all the related articles into five topics and offer an overall understanding for both industry practice and academic study.
6. CONCLUSIONS
By reviewing and analyzing the content of 50 related articles in terms of research topic, contri- bution, and limitations, sample sources, data types, and analysis methods, this study contri- butes to the literature by clearly answering the research question “What have hospitality and tourism researchers done with regard to the use of online reviews?” In practical terms, this article will help industry practitioners and academic researchers in attaining a better understanding of the relationship between online reviews and online buying behavior, and how to deal with online complaints and improve customer satisfac- tion. The study also reveals what can be mined through massive databases of online reviews posted by customers, what motives customers to post reviews, and the role played by online
5.
DISCUSSION AND IMPLICATIONS
It is apparent that hospitality and tourism studies on online reviews have taken a keen interest in the relationship between online reviews and online buying behavior. The impact of online reviews on online sales and consumer decision making are the most researched areas due to their importance to hospitality and tour- ism. Most of these articles have a fairly strong industry focus, primarily the hotel industry, and offer sound practical suggestions and implica- tions based on their findings. Another popular field is customer satisfaction and the manage- ment of online reviews, which is related to online sales. Investigating the implications con- cerning the use of artificial intelligence technol- ogy on opinion mining/sentiment analysis of online reviews was popular for a time. However, this hot trend vanished due to a lack of innovative methods. The methods applied in this field are alike and this makes it difficult to move forward unless a revolutionary method appears. The motivations for posting, reading, and sharing reviews as well as the role of reviews on e-commerce platforms have been relatively less studied. Data sources from the USA and China are the most popular, and sec- ondary data retrieved from e-commerce plat- forms are well employed to make empirical analysis and text mining.
From the perspective of consumers, checking online reviews is regarded as an indispensable process when buying online. Later, online reviews are regenerated by these online buyers expressing their satisfaction or complaining. Through this cycle, online reviews become more popular and influential on the platform, actually work as the most important role between buyers and sellers. From the perspec- tive of suppliers, an online reputation created
Schuckert, Liu, and Law 617

618 JOURNAL OF TRAVEL & TOURISM MARKETING
reviews on e-commerce platforms. In brief, online reviews appear to be a strategic tool that plays an important role in hospitality and tourism manage- ment, especially in promotion, online sales, and reputation management. We expect that in reading this study scholars will be able to gain a clear understanding of what previous researchers have done on hospitality and tourism online reviews, and the limitations of current research. We also believe that this contribution will enable managers to gain an overview of the main findings of recent research focusing on online reviews, and that this will inspire them to think about adapting or improving their e-commerce strategy. Based on the limitations of extant studies and the current operating state, we propose some directions for future research as follows.
Firstly, more valid and reasonable proxy vari- ables should be found to measure online sales since the real sales of each hotel or restaurant are commercial a secret that researchers cannot obtain, while in academic studies online sales are frequently required. The number of online reviews is always regarded as the amount of online sales in previous studies, which is a very inaccurate assumption.
Secondly, research on the method by which to scale the performance of online management needs to be conducted in order to reveal whether and where online management really works and to what extent the supplier gains profit from it. The effect of reviews manage- ment needs to be quantified, thus allowing man- agers to decide whether and to what extent it is necessary to engage in reviews management.
Thirdly, increasing the accuracy and effi- ciency of opinion mining and sentiment analy- sis by advancing the methods applied or designing a completely new approach seems necessary to boost the applicability of artificial intelligence to online reviews. Data mining may be more important than sentiment analysis, which classifies reviews into positive or nega- tive, because most hospitality and tourism- related review functions operate together with ratings which directly reflect sentiment and are more accurate since they are quantified. Data mining can make up the quantitative part and may find product defects or service failures among numerous online reviews.
Fourthly, combining both theory building and empirical study in the exploration of psy- chological behavior when focusing on motiva- tion research or on discovering cultural differences in online reviews or ratings also seems to be a promising line for future research. For example, if the findings show that oriental customers value service quality more than occi- dental customers, hotels may use discriminatory levels of service passion: more enthusiastic towards Eastern guests, and business as usual for Westerners.
A major limitation of this study is that our framework may be not impeccable and may need to be improved. Since online reviewing is a relatively new phenomenon in academic research, there are only 50 related articles, which is a small number for classification. However, the authors are confident that more articles will be published in the future. Another limitation is that the authors focused solely on English-language articles in academic journals, neglecting articles in other languages and arti- cles in books or conference proceedings. Further examination of Chinese articles, which have been proliferating rapidly, should be con- ducted in the future (Zhong et al., 2013).


